{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "find_lr_vgg.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "C14qc_v-CB60",
        "szcydVn7CdM-",
        "Gqs_p_mLECnW",
        "I1p7ClQOERJh"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylv3VBMHA_hu"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JIWi4hbZfiX"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "import random\n",
        "import sys\n",
        "import os\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPKVHVBsOOX3",
        "outputId": "f9725453-63ea-47b5-93cf-5ff21509a217"
      },
      "source": [
        "# mount drive folder and import custom modules\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Shark_Classification')\n",
        "\n",
        "from architectures.smallvggnet import SmallVGGNet\n",
        "from keras_callbacks.keras_callbacks import LearningRateScreening"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeaeP3A5BLOT"
      },
      "source": [
        "## **Function to List Files in Directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1EjaMbMTksF"
      },
      "source": [
        "file_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
        "keep_labels     = ['great_white_shark','hammerhead_shark']\n",
        "\n",
        "def list_files(indir=os.getcwd(),valid_extensions=file_extensions,valid_labels=keep_labels):\n",
        "    for (rootdir,dirs,files) in os.walk(indir):\n",
        "        for filename in files:\n",
        "            # determine the file extension of the current file\n",
        "            ext = filename[filename.rfind(\".\"):].lower()\n",
        "            \n",
        "            # check to see if the file is an image and should be processed\n",
        "            if valid_extensions is None or ext.endswith(valid_extensions):\n",
        "                \n",
        "                # construct the path to the image and yield it\n",
        "                imagePath = os.path.join(rootdir, filename)\n",
        "                \n",
        "                # yield the path if the label should not be dropped \n",
        "                if imagePath.split(os.path.sep)[-2] in valid_labels:\n",
        "                    yield imagePath\n",
        "            \n",
        "    return"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Lt0SwuBc28"
      },
      "source": [
        "## **Read Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW1DPwj5BcCL",
        "outputId": "7ddfd3ae-954f-465a-bf7d-7f29c5418deb"
      },
      "source": [
        "dataset_path = os.path.join(sys.path[0],'sharks')\n",
        "output_path= os.path.join(sys.path[0],\"output\")\n",
        "    \n",
        "#obtain image paths and ramdomize it\n",
        "image_paths = list(list_files(dataset_path))\n",
        "random.seed(42)\n",
        "random.shuffle(image_paths)\n",
        "    \n",
        "# initialize data and labels list\n",
        "data, labels, count, max_count = [],[],0,-1\n",
        "\n",
        "print(\"[INFO] Reading images from disk. This may take a while ... \")    \n",
        "for i in image_paths:\n",
        "\n",
        "    # load the image  and store the image in the data list\n",
        "    image = cv2.imread(i)\n",
        "    image = cv2.resize(image, (64, 64))\n",
        "    data.append(image)\n",
        "       \n",
        "    label = i.split(os.path.sep)[-2]\n",
        "    labels.append(label)\n",
        "        \n",
        "    count+=1\n",
        "    if count==max_count: break\n",
        "\n",
        "# print label count\n",
        "label_list = os.listdir(dataset_path)\n",
        "for l in label_list: print(\"label: {} counts: {}\".format(l,labels.count(l)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Reading images from disk. This may take a while ... \n",
            "label: great_white_shark counts: 928\n",
            "label: mako counts: 0\n",
            "label: tiger_shark counts: 0\n",
            "label: hammerhead_shark counts: 744\n",
            "label: whale_shark counts: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C14qc_v-CB60"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szcydVn7CdM-"
      },
      "source": [
        "### **Scale Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asDzKbgeCCF9"
      },
      "source": [
        "# scale the raw pixel intensities to the range [0, 1]\n",
        "data = np.array(data,dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqs_p_mLECnW"
      },
      "source": [
        "### **Train/Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foIklGOKEDFg"
      },
      "source": [
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,labels, test_size=0.25, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1p7ClQOERJh"
      },
      "source": [
        "###**One Hot-Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feCHcmWhEQMQ"
      },
      "source": [
        "original_classes = trainY\n",
        "    \n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "trainY = label_encoder.fit_transform(trainY)\n",
        "testY  = label_encoder.transform(testY)\n",
        "    \n",
        "#print(original_classes[0:10])\n",
        "#print(trainY[0:10])\n",
        "u, indices =np.unique(trainY,return_index=True)\n",
        "classes = [original_classes[i] for i in indices]\n",
        "#print(classes)\n",
        "  \n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "trainY = trainY.reshape(len(trainY), 1)\n",
        "trainY = onehot_encoder.fit_transform(trainY)\n",
        "testY = testY.reshape(len(testY), 1)\n",
        "testY = onehot_encoder.transform(testY)\n",
        "#print(trainY[0:10])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsndM3gmEmk3"
      },
      "source": [
        "###**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYwIhYXHEsIN"
      },
      "source": [
        "# construct the image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
        "                          height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                          horizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dO-evtQ0MXC"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT-I39MJE0mA"
      },
      "source": [
        "##**Define Model and Compile**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUjDLo-gFJXs",
        "outputId": "d5a9c1f3-9106-474f-e1f7-10cd1398057d"
      },
      "source": [
        "# initialize our VGG-like Convolutional Neural Network\n",
        "model = SmallVGGNet.build(width=64, height=64, depth=3,classes=len(classes))\n",
        "\n",
        "# initialize our initial learning rate and # of epochs to train for\n",
        "INIT_LR = 1e-9\n",
        "EPOCHS = 100\n",
        "BS = 128\n",
        "STEP_EPOCH = len(trainX) // BS\n",
        "N_BATCH_UPDATES = STEP_EPOCH*EPOCHS\n",
        "\n",
        "# init learning rate screening callback\n",
        "lr_screening = LearningRateScreening(max_lr = 10, n_batch_updates = N_BATCH_UPDATES)\n",
        "    \n",
        "# compile the model using SGD as our optimizer and categorical\n",
        "# cross-entropy loss (you'll want to use binary_crossentropy\n",
        "# for 2-class classification)\n",
        "print(\"[INFO] training network...\")\n",
        "opt = SGD(learning_rate=INIT_LR)#,decay=INIT_LR/EPOCHS - 0.005\n",
        "#opt = Adam(learning_rate=INIT_LR)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFEDrlXfFZhd"
      },
      "source": [
        "##**Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-aJgCaHFdWj",
        "outputId": "6f2e7094-4f87-4dba-99aa-54544a013a70"
      },
      "source": [
        "# train the network\n",
        "#math.floor(math.log(lr,10))\n",
        "H = model.fit(x=aug.flow(trainX, trainY, batch_size=BS),\n",
        "              validation_data=(testX, testY), steps_per_epoch=STEP_EPOCH,\n",
        "              epochs=EPOCHS,callbacks=[lr_screening])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/9 [==>...........................] - ETA: 12s - loss: 0.9982 - accuracy: 0.5078 - learning rate: 9.999999717180685e-10\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0171 - accuracy: 0.5391  - learning rate: 1.025914353469659e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0233 - accuracy: 0.5495 - learning rate: 1.0525003091288454e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0303 - accuracy: 0.5410 - learning rate: 1.0797751581748116e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0420 - accuracy: 0.5344 - learning rate: 1.1077568862205567e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0485 - accuracy: 0.5247 - learning rate: 1.1364637009236844e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0682 - accuracy: 0.5161 - learning rate: 1.1659144760756135e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0829 - accuracy: 0.5060 - learning rate: 1.1961284185346699e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0783 - accuracy: 0.5036 - learning rate: 1.227125290270692e-09\n",
            "9/9 [==============================] - 4s 292ms/step - loss: 1.0783 - accuracy: 0.5036 - val_loss: 0.6954 - val_accuracy: 0.5311\n",
            "Epoch 2/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0842 - accuracy: 0.5156 - learning rate: 1.2589254083650303e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.9934 - accuracy: 0.5352 - learning rate: 1.291549645010548e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.9638 - accuracy: 0.5547 - learning rate: 1.3250193164893176e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.9939 - accuracy: 0.5312 - learning rate: 1.3593564052172269e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0027 - accuracy: 0.5277 - learning rate: 1.3945832266770708e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0211 - accuracy: 0.5243 - learning rate: 1.430722984530064e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0215 - accuracy: 0.5195 - learning rate: 1.4677992155043285e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0285 - accuracy: 0.5140 - learning rate: 1.505836344506406e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0186 - accuracy: 0.5133 - learning rate: 1.5448591295097458e-09\n",
            "9/9 [==============================] - 2s 236ms/step - loss: 1.0186 - accuracy: 0.5133 - val_loss: 0.7041 - val_accuracy: 0.5311\n",
            "Epoch 3/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 1.1189 - accuracy: 0.4412 - learning rate: 1.5848932166662166e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1238 - accuracy: 0.4522 - learning rate: 1.6259646962168972e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.1295 - accuracy: 0.4665 - learning rate: 1.668100546581286e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1192 - accuracy: 0.4650 - learning rate: 1.7113283012903935e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.1026 - accuracy: 0.4609 - learning rate: 1.7556762710313478e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0837 - accuracy: 0.4717 - learning rate: 1.8011735436473941e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0731 - accuracy: 0.4747 - learning rate: 1.8478497620932899e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0725 - accuracy: 0.4749 - learning rate: 1.8957355685245147e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0675 - accuracy: 0.4751 - learning rate: 1.944862271230363e-09\n",
            "9/9 [==============================] - 2s 237ms/step - loss: 1.0675 - accuracy: 0.4751 - val_loss: 0.7150 - val_accuracy: 0.5311\n",
            "Epoch 4/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1134 - accuracy: 0.5312 - learning rate: 1.9952621777008517e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1109 - accuracy: 0.5195 - learning rate: 2.0469681505375092e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0797 - accuracy: 0.5104 - learning rate: 2.1000139405202845e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0585 - accuracy: 0.5103 - learning rate: 2.1544344086521505e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0506 - accuracy: 0.5065 - learning rate: 2.2102653041145004e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0498 - accuracy: 0.5013 - learning rate: 2.267542820177937e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0331 - accuracy: 0.5057 - learning rate: 2.326304704425297e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0218 - accuracy: 0.5070 - learning rate: 2.3865893705732333e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0209 - accuracy: 0.5098 - learning rate: 2.448436342561422e-09\n",
            "9/9 [==============================] - 2s 232ms/step - loss: 1.0209 - accuracy: 0.5098 - val_loss: 0.7276 - val_accuracy: 0.5311\n",
            "Epoch 5/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0237 - accuracy: 0.5469 - learning rate: 2.5118860325079595e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0405 - accuracy: 0.5352 - learning rate: 2.576979962753967e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0448 - accuracy: 0.5052 - learning rate: 2.6437607658635898e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0450 - accuracy: 0.5098 - learning rate: 2.7122721846239983e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0514 - accuracy: 0.5147 - learning rate: 2.7825590720453874e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0575 - accuracy: 0.5135 - learning rate: 2.8546673913609766e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0618 - accuracy: 0.5149 - learning rate: 2.92864421602701e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0636 - accuracy: 0.5160 - learning rate: 3.004538173811966e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0654 - accuracy: 0.5080 - learning rate: 3.082398780662743e-09\n",
            "9/9 [==============================] - 2s 234ms/step - loss: 1.0654 - accuracy: 0.5080 - val_loss: 0.7395 - val_accuracy: 0.5311\n",
            "Epoch 6/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0339 - accuracy: 0.4844 - learning rate: 3.1622771068384736e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0433 - accuracy: 0.4844 - learning rate: 3.2442255548659205e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0478 - accuracy: 0.4792 - learning rate: 3.3282976374948703e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0430 - accuracy: 0.4863 - learning rate: 3.4145484217873445e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0303 - accuracy: 0.4984 - learning rate: 3.503034307072994e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0408 - accuracy: 0.4987 - learning rate: 3.5938132469937045e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0444 - accuracy: 0.4978 - learning rate: 3.6869445274589907e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0492 - accuracy: 0.4990 - learning rate: 3.782489432779812e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0643 - accuracy: 0.4939 - learning rate: 3.880510135445547e-09\n",
            "9/9 [==============================] - 2s 238ms/step - loss: 1.0643 - accuracy: 0.4939 - val_loss: 0.7511 - val_accuracy: 0.5311\n",
            "Epoch 7/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1555 - accuracy: 0.4531 - learning rate: 3.981071028391625e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1649 - accuracy: 0.4727 - learning rate: 4.084237836821103e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.1278 - accuracy: 0.4844 - learning rate: 4.190078062293878e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1027 - accuracy: 0.5020 - learning rate: 4.298661426815897e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0977 - accuracy: 0.5081 - learning rate: 4.4100585405715265e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0938 - accuracy: 0.5067 - learning rate: 4.524342234191181e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0987 - accuracy: 0.4977 - learning rate: 4.6415875587513256e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.1004 - accuracy: 0.4890 - learning rate: 4.761871341685264e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0823 - accuracy: 0.4991 - learning rate: 4.8852721867831406e-09\n",
            "9/9 [==============================] - 2s 234ms/step - loss: 1.0823 - accuracy: 0.4991 - val_loss: 0.7610 - val_accuracy: 0.5311\n",
            "Epoch 8/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9933 - accuracy: 0.5703 - learning rate: 5.011870918281147e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0069 - accuracy: 0.5312 - learning rate: 5.141750580861526e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0130 - accuracy: 0.5182 - learning rate: 5.2749959955633585e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0064 - accuracy: 0.5254 - learning rate: 5.411694203871775e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0187 - accuracy: 0.5266 - learning rate: 5.551934911807166e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0408 - accuracy: 0.5156 - learning rate: 5.69580960174676e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0529 - accuracy: 0.5134 - learning rate: 5.8434128646922545e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0523 - accuracy: 0.5234 - learning rate: 5.994841068002188e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0523 - accuracy: 0.5231 - learning rate: 6.1501936876595664e-09\n",
            "9/9 [==============================] - 2s 232ms/step - loss: 1.0523 - accuracy: 0.5231 - val_loss: 0.7695 - val_accuracy: 0.5311\n",
            "Epoch 9/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 1.0275 - accuracy: 0.5784 - learning rate: 6.309571976004236e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0151 - accuracy: 0.5870 - learning rate: 6.47308073808972e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0241 - accuracy: 0.5503 - learning rate: 6.6408265553263846e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0536 - accuracy: 0.5350 - learning rate: 6.812919561838271e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0411 - accuracy: 0.5261 - learning rate: 6.989472112195472e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0471 - accuracy: 0.5121 - learning rate: 7.170599669592548e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0443 - accuracy: 0.5046 - learning rate: 7.356421249937739e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0450 - accuracy: 0.4980 - learning rate: 7.547058089585335e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0493 - accuracy: 0.4991 - learning rate: 7.742634977603302e-09\n",
            "9/9 [==============================] - 2s 236ms/step - loss: 1.0493 - accuracy: 0.4991 - val_loss: 0.7761 - val_accuracy: 0.5311\n",
            "Epoch 10/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9693 - accuracy: 0.5234 - learning rate: 7.943280699862498e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0057 - accuracy: 0.5039 - learning rate: 8.149125818590619e-09\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.9861 - accuracy: 0.5104 - learning rate: 8.360305336907459e-09\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0043 - accuracy: 0.5098 - learning rate: 8.576956922468071e-09\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0016 - accuracy: 0.5109 - learning rate: 8.799223571998027e-09\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9894 - accuracy: 0.5169 - learning rate: 9.027250058579739e-09\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0073 - accuracy: 0.5123 - learning rate: 9.261185596187715e-09\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0193 - accuracy: 0.5100 - learning rate: 9.501182951510145e-09\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0325 - accuracy: 0.5009 - learning rate: 9.747400220305735e-09\n",
            "9/9 [==============================] - 2s 237ms/step - loss: 1.0325 - accuracy: 0.5009 - val_loss: 0.7808 - val_accuracy: 0.5311\n",
            "Epoch 11/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1597 - accuracy: 0.4453 - learning rate: 9.999998162868451e-09\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1815 - accuracy: 0.4258 - learning rate: 1.0259141980384356e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.1576 - accuracy: 0.4401 - learning rate: 1.0525001314931615e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1104 - accuracy: 0.4648 - learning rate: 1.0797750249480487e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.1173 - accuracy: 0.4719 - learning rate: 1.1077567307893332e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.1143 - accuracy: 0.4766 - learning rate: 1.136463545492461e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.1090 - accuracy: 0.4799 - learning rate: 1.1659142984399296e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.1000 - accuracy: 0.4844 - learning rate: 1.1961282631034464e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.4893 - learning rate: 1.227125157043929e-08\n",
            "9/9 [==============================] - 2s 237ms/step - loss: 1.0987 - accuracy: 0.4893 - val_loss: 0.7837 - val_accuracy: 0.5311\n",
            "Epoch 12/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0646 - accuracy: 0.5234 - learning rate: 1.2589253195471883e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1013 - accuracy: 0.4922 - learning rate: 1.2915495339882455e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0806 - accuracy: 0.5052 - learning rate: 1.3250192054670151e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1270 - accuracy: 0.4805 - learning rate: 1.359356271990464e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.1185 - accuracy: 0.4875 - learning rate: 1.3945831156547683e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.1293 - accuracy: 0.4805 - learning rate: 1.4307228290988405e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.1310 - accuracy: 0.4754 - learning rate: 1.4677991266864865e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.1165 - accuracy: 0.4795 - learning rate: 1.505836166870722e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1014 - accuracy: 0.4867 - learning rate: 1.544858996282983e-08\n",
            "9/9 [==============================] - 2s 232ms/step - loss: 1.1014 - accuracy: 0.4867 - val_loss: 0.7858 - val_accuracy: 0.5311\n",
            "Epoch 13/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0857 - accuracy: 0.4375 - learning rate: 1.5848931056439142e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0523 - accuracy: 0.4727 - learning rate: 1.6259646073990552e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0583 - accuracy: 0.4792 - learning rate: 1.668100502172365e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0612 - accuracy: 0.4707 - learning rate: 1.711328323494854e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0332 - accuracy: 0.4828 - learning rate: 1.7556763154402688e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0466 - accuracy: 0.4909 - learning rate: 1.8011736102607756e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0595 - accuracy: 0.4922 - learning rate: 1.8478498731155923e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0521 - accuracy: 0.4960 - learning rate: 1.8957356573423567e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0567 - accuracy: 0.4964 - learning rate: 1.944862404457126e-08\n",
            "9/9 [==============================] - 2s 237ms/step - loss: 1.0567 - accuracy: 0.4964 - val_loss: 0.7861 - val_accuracy: 0.5311\n",
            "Epoch 14/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1388 - accuracy: 0.4766 - learning rate: 1.9952622665186937e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1218 - accuracy: 0.4727 - learning rate: 2.0469682837642722e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0832 - accuracy: 0.4948 - learning rate: 2.1000142069738104e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1209 - accuracy: 0.4980 - learning rate: 2.1544346751056764e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.1117 - accuracy: 0.4953 - learning rate: 2.2102655705680263e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0989 - accuracy: 0.5065 - learning rate: 2.2675431310403837e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0800 - accuracy: 0.5112 - learning rate: 2.326305015287744e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0691 - accuracy: 0.5200 - learning rate: 2.386589770253522e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0618 - accuracy: 0.5222 - learning rate: 2.448436653423869e-08\n",
            "9/9 [==============================] - 2s 237ms/step - loss: 1.0618 - accuracy: 0.5222 - val_loss: 0.7849 - val_accuracy: 0.5311\n",
            "Epoch 15/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0038 - accuracy: 0.5078 - learning rate: 2.5118863433704064e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0241 - accuracy: 0.4727 - learning rate: 2.5769802292074928e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0154 - accuracy: 0.4974 - learning rate: 2.6437611211349576e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0095 - accuracy: 0.5137 - learning rate: 2.7122725398953662e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0212 - accuracy: 0.5047 - learning rate: 2.7825594273167553e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0216 - accuracy: 0.5039 - learning rate: 2.8546676134055815e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0134 - accuracy: 0.5067 - learning rate: 2.9286445268894568e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0356 - accuracy: 0.5059 - learning rate: 3.004538484674413e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0207 - accuracy: 0.5148 - learning rate: 3.082399047116269e-08\n",
            "9/9 [==============================] - 2s 238ms/step - loss: 1.0207 - accuracy: 0.5148 - val_loss: 0.7828 - val_accuracy: 0.5311\n",
            "Epoch 16/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0282 - accuracy: 0.5469 - learning rate: 3.1622775509276835e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0803 - accuracy: 0.4883 - learning rate: 3.2442258657283674e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0342 - accuracy: 0.4948 - learning rate: 3.328297992766238e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0348 - accuracy: 0.5039 - learning rate: 3.4145486438319494e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0537 - accuracy: 0.4953 - learning rate: 3.503034662344362e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0573 - accuracy: 0.4948 - learning rate: 3.5938136022650724e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0559 - accuracy: 0.5000 - learning rate: 3.6869451491838845e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0677 - accuracy: 0.4951 - learning rate: 3.782490054504706e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0680 - accuracy: 0.4957 - learning rate: 3.880510845988283e-08\n",
            "9/9 [==============================] - 2s 238ms/step - loss: 1.0680 - accuracy: 0.4957 - val_loss: 0.7798 - val_accuracy: 0.5311\n",
            "Epoch 17/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0203 - accuracy: 0.5234 - learning rate: 3.981071827752203e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0548 - accuracy: 0.5273 - learning rate: 4.084238724999523e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0432 - accuracy: 0.5260 - learning rate: 4.19007903929014e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0394 - accuracy: 0.5312 - learning rate: 4.298662403812159e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0718 - accuracy: 0.5078 - learning rate: 4.410059517567788e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0667 - accuracy: 0.5078 - learning rate: 4.5243435664588105e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0635 - accuracy: 0.5123 - learning rate: 4.641589157472481e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0454 - accuracy: 0.5156 - learning rate: 4.761873029224262e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0428 - accuracy: 0.5148 - learning rate: 4.885274051957822e-08\n",
            "9/9 [==============================] - 2s 240ms/step - loss: 1.0428 - accuracy: 0.5148 - val_loss: 0.7766 - val_accuracy: 0.5311\n",
            "Epoch 18/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9853 - accuracy: 0.5156 - learning rate: 5.0118728722736705e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0303 - accuracy: 0.4883 - learning rate: 5.1417522684005235e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0069 - accuracy: 0.4974 - learning rate: 5.274997505466672e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0338 - accuracy: 0.4922 - learning rate: 5.411695624957247e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0429 - accuracy: 0.5016 - learning rate: 5.5519361552569535e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0461 - accuracy: 0.5052 - learning rate: 5.695811111650073e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0494 - accuracy: 0.4955 - learning rate: 5.843414285777726e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0542 - accuracy: 0.4932 - learning rate: 5.994842666723343e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0581 - accuracy: 0.4905 - learning rate: 6.150195019927196e-08\n",
            "9/9 [==============================] - 2s 243ms/step - loss: 1.0581 - accuracy: 0.4905 - val_loss: 0.7741 - val_accuracy: 0.5311\n",
            "Epoch 19/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9938 - accuracy: 0.4531 - learning rate: 6.309573308271865e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.9935 - accuracy: 0.4883 - learning rate: 6.473081981539508e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0061 - accuracy: 0.4870 - learning rate: 6.640827621140488e-08\n",
            "4/9 [============>.................] - ETA: 0s - loss: 0.9654 - accuracy: 0.5165 - learning rate: 6.812920361198849e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9787 - accuracy: 0.5098 - learning rate: 6.989473178009575e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0212 - accuracy: 0.4960 - learning rate: 7.170601179495861e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0312 - accuracy: 0.4908 - learning rate: 7.356423026294578e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0341 - accuracy: 0.4910 - learning rate: 7.547060221213542e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0475 - accuracy: 0.4885 - learning rate: 7.742637819774245e-08\n",
            "9/9 [==============================] - 2s 234ms/step - loss: 1.0475 - accuracy: 0.4885 - val_loss: 0.7713 - val_accuracy: 0.5311\n",
            "Epoch 20/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1204 - accuracy: 0.5391 - learning rate: 7.943283719669125e-08\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1092 - accuracy: 0.5117 - learning rate: 8.149128660761562e-08\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0906 - accuracy: 0.5052 - learning rate: 8.360308356714086e-08\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1241 - accuracy: 0.4922 - learning rate: 8.576960652817434e-08\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0868 - accuracy: 0.5000 - learning rate: 8.799226947076022e-08\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0746 - accuracy: 0.5067 - learning rate: 9.027253611293418e-08\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0596 - accuracy: 0.5103 - learning rate: 9.261189148901394e-08\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0615 - accuracy: 0.5010 - learning rate: 9.501187037130876e-08\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0702 - accuracy: 0.5027 - learning rate: 9.747404305926466e-08\n",
            "9/9 [==============================] - 2s 235ms/step - loss: 1.0702 - accuracy: 0.5027 - val_loss: 0.7687 - val_accuracy: 0.5311\n",
            "Epoch 21/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9753 - accuracy: 0.5000 - learning rate: 1.0000002248489182e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0421 - accuracy: 0.5039 - learning rate: 1.0259145710733719e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0382 - accuracy: 0.4974 - learning rate: 1.0525005222916661e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0146 - accuracy: 0.5000 - learning rate: 1.0797754157465533e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0303 - accuracy: 0.4984 - learning rate: 1.1077570860607011e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0358 - accuracy: 0.4909 - learning rate: 1.1364639362909656e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0308 - accuracy: 0.4922 - learning rate: 1.1659146537112974e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0299 - accuracy: 0.5020 - learning rate: 1.1961286361383827e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0281 - accuracy: 0.5139 - learning rate: 1.2271254945517285e-07\n",
            "9/9 [==============================] - 2s 242ms/step - loss: 1.0281 - accuracy: 0.5139 - val_loss: 0.7655 - val_accuracy: 0.5311\n",
            "Epoch 22/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1144 - accuracy: 0.4297 - learning rate: 1.2589256925821246e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0954 - accuracy: 0.4492 - learning rate: 1.2915499780774553e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0872 - accuracy: 0.4740 - learning rate: 1.3250196673197934e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0810 - accuracy: 0.4941 - learning rate: 1.3593566450254002e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0889 - accuracy: 0.4922 - learning rate: 1.394583506453273e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0667 - accuracy: 0.5000 - learning rate: 1.4307232731880504e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0681 - accuracy: 0.4931 - learning rate: 1.4677995352485596e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0567 - accuracy: 0.4970 - learning rate: 1.5058365931963635e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0594 - accuracy: 0.4929 - learning rate: 1.544859458135761e-07\n",
            "9/9 [==============================] - 2s 239ms/step - loss: 1.0594 - accuracy: 0.4929 - val_loss: 0.7623 - val_accuracy: 0.5311\n",
            "Epoch 23/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 1.0709 - accuracy: 0.4902 - learning rate: 1.5848935674966924e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0980 - accuracy: 0.5000 - learning rate: 1.6259650692518335e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0638 - accuracy: 0.5223 - learning rate: 1.6681009640251432e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0550 - accuracy: 0.5206 - learning rate: 1.7113286787662219e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0596 - accuracy: 0.5130 - learning rate: 1.7556766351845e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0808 - accuracy: 0.5027 - learning rate: 1.8011738234235963e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0742 - accuracy: 0.5011 - learning rate: 1.847850086278413e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0778 - accuracy: 0.4980 - learning rate: 1.8957359770865878e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0923 - accuracy: 0.4902 - learning rate: 1.944862759728494e-07\n",
            "9/9 [==============================] - 2s 244ms/step - loss: 1.0923 - accuracy: 0.4902 - val_loss: 0.7597 - val_accuracy: 0.5311\n",
            "Epoch 24/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1122 - accuracy: 0.4609 - learning rate: 1.995262692844335e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0606 - accuracy: 0.5174 - learning rate: 2.0469686035085033e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0487 - accuracy: 0.5168 - learning rate: 2.100014455663768e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0484 - accuracy: 0.5165 - learning rate: 2.154435065904181e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0712 - accuracy: 0.5016 - learning rate: 2.2102658192579838e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0721 - accuracy: 0.4946 - learning rate: 2.2675435218388884e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0683 - accuracy: 0.5000 - learning rate: 2.3263054060862487e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0747 - accuracy: 0.4950 - learning rate: 2.386590267633437e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0803 - accuracy: 0.4849 - learning rate: 2.448437328439468e-07\n",
            "9/9 [==============================] - 2s 238ms/step - loss: 1.0803 - accuracy: 0.4849 - val_loss: 0.7585 - val_accuracy: 0.5311\n",
            "Epoch 25/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9898 - accuracy: 0.5391 - learning rate: 2.511887089440279e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0380 - accuracy: 0.5195 - learning rate: 2.576981046331639e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0534 - accuracy: 0.4948 - learning rate: 2.6437618316776934e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0560 - accuracy: 0.5020 - learning rate: 2.712273214910965e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0343 - accuracy: 0.5141 - learning rate: 2.7825601023323543e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0611 - accuracy: 0.4883 - learning rate: 2.8546682528940437e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0671 - accuracy: 0.4911 - learning rate: 2.928645130850782e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0482 - accuracy: 0.5098 - learning rate: 3.0045390531086014e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0524 - accuracy: 0.5071 - learning rate: 3.0823997576590045e-07\n",
            "9/9 [==============================] - 2s 236ms/step - loss: 1.0524 - accuracy: 0.5071 - val_loss: 0.7573 - val_accuracy: 0.5311\n",
            "Epoch 26/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0450 - accuracy: 0.5547 - learning rate: 3.162278119361872e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0079 - accuracy: 0.5391 - learning rate: 3.244226434162556e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0557 - accuracy: 0.5052 - learning rate: 3.3282984190918796e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0341 - accuracy: 0.5254 - learning rate: 3.414549212266138e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0424 - accuracy: 0.5219 - learning rate: 3.5030350886700035e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0689 - accuracy: 0.5040 - learning rate: 3.593814028590714e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0691 - accuracy: 0.5023 - learning rate: 3.686945433400979e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0609 - accuracy: 0.5010 - learning rate: 3.7824904097760736e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0638 - accuracy: 0.5000 - learning rate: 3.880511201259651e-07\n",
            "9/9 [==============================] - 2s 236ms/step - loss: 1.0638 - accuracy: 0.5000 - val_loss: 0.7568 - val_accuracy: 0.5311\n",
            "Epoch 27/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0523 - accuracy: 0.4922 - learning rate: 3.981072325132118e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0662 - accuracy: 0.4961 - learning rate: 4.084239151325164e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0486 - accuracy: 0.5156 - learning rate: 4.1900796077243285e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0570 - accuracy: 0.5117 - learning rate: 4.298662759083527e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0593 - accuracy: 0.5063 - learning rate: 4.4100599438934296e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0592 - accuracy: 0.5065 - learning rate: 4.5243439217301784e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0493 - accuracy: 0.5078 - learning rate: 4.6415894416895753e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0531 - accuracy: 0.5060 - learning rate: 4.7618732423870824e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0529 - accuracy: 0.5098 - learning rate: 4.885274051957822e-07\n",
            "9/9 [==============================] - 2s 238ms/step - loss: 1.0529 - accuracy: 0.5098 - val_loss: 0.7569 - val_accuracy: 0.5311\n",
            "Epoch 28/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0158 - accuracy: 0.5547 - learning rate: 5.01187287227367e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0184 - accuracy: 0.5469 - learning rate: 5.141752126291976e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0165 - accuracy: 0.5312 - learning rate: 5.274997647575219e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0213 - accuracy: 0.5215 - learning rate: 5.411695838120067e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0134 - accuracy: 0.5203 - learning rate: 5.551936510528321e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0003 - accuracy: 0.5286 - learning rate: 5.695811182704347e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0109 - accuracy: 0.5257 - learning rate: 5.843414783157641e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0253 - accuracy: 0.5130 - learning rate: 5.994843377266079e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0232 - accuracy: 0.5213 - learning rate: 6.150195872578479e-07\n",
            "9/9 [==============================] - 2s 236ms/step - loss: 1.0232 - accuracy: 0.5213 - val_loss: 0.7584 - val_accuracy: 0.5311\n",
            "Epoch 29/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0580 - accuracy: 0.5234 - learning rate: 6.309574018814601e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0503 - accuracy: 0.5156 - learning rate: 6.473082407865149e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0335 - accuracy: 0.5260 - learning rate: 6.640828473791771e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0463 - accuracy: 0.5254 - learning rate: 6.812921355958679e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0396 - accuracy: 0.5266 - learning rate: 6.989474172769405e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0278 - accuracy: 0.5378 - learning rate: 7.17060174793005e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0241 - accuracy: 0.5379 - learning rate: 7.35642345262022e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0328 - accuracy: 0.5225 - learning rate: 7.547060363322089e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0386 - accuracy: 0.5187 - learning rate: 7.742637535557151e-07\n",
            "9/9 [==============================] - 2s 237ms/step - loss: 1.0386 - accuracy: 0.5187 - val_loss: 0.7601 - val_accuracy: 0.5287\n",
            "Epoch 30/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 1.0849 - accuracy: 0.4216 - learning rate: 7.943282867017842e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0806 - accuracy: 0.4565 - learning rate: 8.149128234435921e-07\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.1161 - accuracy: 0.4525 - learning rate: 8.360307788279897e-07\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0973 - accuracy: 0.4568 - learning rate: 8.576959658057604e-07\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0942 - accuracy: 0.4577 - learning rate: 8.799225952316192e-07\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0836 - accuracy: 0.4596 - learning rate: 9.027252190207946e-07\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0993 - accuracy: 0.4540 - learning rate: 9.26118786992447e-07\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.1081 - accuracy: 0.4529 - learning rate: 9.501185900262499e-07\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1164 - accuracy: 0.4485 - learning rate: 9.7474026006239e-07\n",
            "9/9 [==============================] - 2s 243ms/step - loss: 1.1164 - accuracy: 0.4485 - val_loss: 0.7613 - val_accuracy: 0.5263\n",
            "Epoch 31/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1113 - accuracy: 0.5391 - learning rate: 9.999999974752427e-07\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0498 - accuracy: 0.5234 - learning rate: 1.0259143436996965e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0738 - accuracy: 0.5104 - learning rate: 1.0525002380745718e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0755 - accuracy: 0.5000 - learning rate: 1.0797750746860402e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0836 - accuracy: 0.5000 - learning rate: 1.1077568160544615e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0638 - accuracy: 0.5013 - learning rate: 1.1364636520738713e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0616 - accuracy: 0.4955 - learning rate: 1.1659143410724937e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0645 - accuracy: 0.4961 - learning rate: 1.196128323499579e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0712 - accuracy: 0.4929 - learning rate: 1.227125267178053e-06\n",
            "9/9 [==============================] - 2s 241ms/step - loss: 1.0712 - accuracy: 0.4929 - val_loss: 0.7624 - val_accuracy: 0.5263\n",
            "Epoch 32/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0017 - accuracy: 0.5000 - learning rate: 1.2589254083650303e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0475 - accuracy: 0.4883 - learning rate: 1.2915496654386516e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0745 - accuracy: 0.4844 - learning rate: 1.3250194115244085e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0762 - accuracy: 0.4980 - learning rate: 1.3593564744951436e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0685 - accuracy: 0.4969 - learning rate: 1.3945833643447259e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0763 - accuracy: 0.4883 - learning rate: 1.4307231595012126e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0949 - accuracy: 0.4743 - learning rate: 1.4677993931400124e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0922 - accuracy: 0.4717 - learning rate: 1.5058365079312352e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0838 - accuracy: 0.4740 - learning rate: 1.5448592876055045e-06\n",
            "9/9 [==============================] - 2s 254ms/step - loss: 1.0838 - accuracy: 0.4740 - val_loss: 0.7639 - val_accuracy: 0.5287\n",
            "Epoch 33/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 0.9384 - accuracy: 0.5686 - learning rate: 1.5848933117013075e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.9664 - accuracy: 0.5696 - learning rate: 1.625964841878158e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0076 - accuracy: 0.5335 - learning rate: 1.6681007082297583e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0320 - accuracy: 0.5247 - learning rate: 1.711328422970837e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0307 - accuracy: 0.5212 - learning rate: 1.7556764078108245e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0365 - accuracy: 0.5148 - learning rate: 1.8011736528933397e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0360 - accuracy: 0.5126 - learning rate: 1.847849944169866e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0377 - accuracy: 0.5080 - learning rate: 1.8957357497129124e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0391 - accuracy: 0.5098 - learning rate: 1.944862560776528e-06\n",
            "9/9 [==============================] - 2s 253ms/step - loss: 1.0391 - accuracy: 0.5098 - val_loss: 0.7657 - val_accuracy: 0.5287\n",
            "Epoch 34/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0840 - accuracy: 0.5391 - learning rate: 1.9952624370489502e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0924 - accuracy: 0.5000 - learning rate: 2.0469683477131184e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0898 - accuracy: 0.4860 - learning rate: 2.1000141714466736e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0531 - accuracy: 0.5123 - learning rate: 2.1544346964219585e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0267 - accuracy: 0.5293 - learning rate: 2.2102653929323424e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0194 - accuracy: 0.5270 - learning rate: 2.267543095513247e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0213 - accuracy: 0.5161 - learning rate: 2.326305093447445e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0393 - accuracy: 0.5030 - learning rate: 2.3865898128860863e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0255 - accuracy: 0.5107 - learning rate: 2.448436816848698e-06\n",
            "9/9 [==============================] - 2s 244ms/step - loss: 1.0255 - accuracy: 0.5107 - val_loss: 0.7666 - val_accuracy: 0.5335\n",
            "Epoch 35/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1261 - accuracy: 0.5000 - learning rate: 2.511886577849509e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1441 - accuracy: 0.4531 - learning rate: 2.5769804778974503e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.1100 - accuracy: 0.4688 - learning rate: 2.643761263243505e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1095 - accuracy: 0.4712 - learning rate: 2.7122725896333577e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0905 - accuracy: 0.4788 - learning rate: 2.782559477054747e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.1010 - accuracy: 0.4757 - learning rate: 2.8546676276164362e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0909 - accuracy: 0.4747 - learning rate: 2.9286445624165935e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0938 - accuracy: 0.4739 - learning rate: 3.0045384846744128e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0808 - accuracy: 0.4760 - learning rate: 3.082399189224816e-06\n",
            "9/9 [==============================] - 2s 242ms/step - loss: 1.0808 - accuracy: 0.4760 - val_loss: 0.7683 - val_accuracy: 0.5383\n",
            "Epoch 36/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0922 - accuracy: 0.4219 - learning rate: 3.1622776077711023e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0555 - accuracy: 0.4609 - learning rate: 3.244226036258624e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0993 - accuracy: 0.4581 - learning rate: 3.3282981348747853e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0924 - accuracy: 0.4588 - learning rate: 3.4145489280490438e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0707 - accuracy: 0.4772 - learning rate: 3.503034804452909e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0593 - accuracy: 0.4838 - learning rate: 3.5938137443736196e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0696 - accuracy: 0.4816 - learning rate: 3.6869450923404656e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0812 - accuracy: 0.4709 - learning rate: 3.7824900118721416e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0738 - accuracy: 0.4707 - learning rate: 3.880511030729394e-06\n",
            "9/9 [==============================] - 2s 243ms/step - loss: 1.0738 - accuracy: 0.4707 - val_loss: 0.7693 - val_accuracy: 0.5431\n",
            "Epoch 37/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0453 - accuracy: 0.5391 - learning rate: 3.9810720409150235e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0738 - accuracy: 0.5000 - learning rate: 4.084239208168583e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0429 - accuracy: 0.5078 - learning rate: 4.190079835098004e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0503 - accuracy: 0.5098 - learning rate: 4.298663043300621e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0469 - accuracy: 0.5098 - learning rate: 4.410060228110524e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0586 - accuracy: 0.5054 - learning rate: 4.524344149103854e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0504 - accuracy: 0.5046 - learning rate: 4.641589839593507e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0460 - accuracy: 0.4970 - learning rate: 4.761873697134433e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0522 - accuracy: 0.4964 - learning rate: 4.885274847765686e-06\n",
            "9/9 [==============================] - 2s 241ms/step - loss: 1.0522 - accuracy: 0.4964 - val_loss: 0.7723 - val_accuracy: 0.5431\n",
            "Epoch 38/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0521 - accuracy: 0.5156 - learning rate: 5.011873781768372e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0547 - accuracy: 0.5039 - learning rate: 5.1417532631603535e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0442 - accuracy: 0.5052 - learning rate: 5.2749983296962455e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0465 - accuracy: 0.5078 - learning rate: 5.411696747614769e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0553 - accuracy: 0.5130 - learning rate: 5.551937647396699e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0252 - accuracy: 0.5243 - learning rate: 5.695812433259562e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0290 - accuracy: 0.5138 - learning rate: 5.843415692652343e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0325 - accuracy: 0.5220 - learning rate: 5.994844286760781e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0276 - accuracy: 0.5240 - learning rate: 6.150196895760018e-06\n",
            "9/9 [==============================] - 2s 240ms/step - loss: 1.0276 - accuracy: 0.5240 - val_loss: 0.7751 - val_accuracy: 0.5407\n",
            "Epoch 39/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0176 - accuracy: 0.4922 - learning rate: 6.309575383056654e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0306 - accuracy: 0.4883 - learning rate: 6.47308388579404e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0643 - accuracy: 0.5000 - learning rate: 6.640829724346986e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0225 - accuracy: 0.5254 - learning rate: 6.812922492827056e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0202 - accuracy: 0.5109 - learning rate: 6.9894749685772695e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0451 - accuracy: 0.4987 - learning rate: 7.170602657424752e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0555 - accuracy: 0.4966 - learning rate: 7.356424248428084e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0506 - accuracy: 0.4910 - learning rate: 7.547061159129953e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0425 - accuracy: 0.4973 - learning rate: 7.742638445051853e-06\n",
            "9/9 [==============================] - 2s 261ms/step - loss: 1.0425 - accuracy: 0.4973 - val_loss: 0.7780 - val_accuracy: 0.5407\n",
            "Epoch 40/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1036 - accuracy: 0.4453 - learning rate: 7.943283890199382e-06\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.1026 - accuracy: 0.4570 - learning rate: 8.149128916556947e-06\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.1536 - accuracy: 0.4557 - learning rate: 8.36030812934041e-06\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1326 - accuracy: 0.4629 - learning rate: 8.576959771744441e-06\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.1238 - accuracy: 0.4547 - learning rate: 8.799226634437218e-06\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.1096 - accuracy: 0.4609 - learning rate: 9.027253327076323e-06\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.1078 - accuracy: 0.4688 - learning rate: 9.261189006792847e-06\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0938 - accuracy: 0.4756 - learning rate: 9.501186468696687e-06\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0924 - accuracy: 0.4748 - learning rate: 9.747403964865953e-06\n",
            "9/9 [==============================] - 2s 252ms/step - loss: 1.0924 - accuracy: 0.4748 - val_loss: 0.7808 - val_accuracy: 0.5359\n",
            "Epoch 41/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 1.0487 - accuracy: 0.5000 - learning rate: 1.0000001566368155e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0261 - accuracy: 0.5087 - learning rate: 1.025914571073372e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0026 - accuracy: 0.5140 - learning rate: 1.0525004654482473e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0317 - accuracy: 0.4979 - learning rate: 1.0797753020597156e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0266 - accuracy: 0.4935 - learning rate: 1.1077569979534019e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0260 - accuracy: 0.4919 - learning rate: 1.1364638339728117e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0248 - accuracy: 0.4989 - learning rate: 1.1659145457088016e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0384 - accuracy: 0.4920 - learning rate: 1.1961285053985193e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0373 - accuracy: 0.4938 - learning rate: 1.2271254490769934e-05\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 1.0373 - accuracy: 0.4938 - val_loss: 0.7827 - val_accuracy: 0.5335\n",
            "Epoch 42/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1022 - accuracy: 0.4922 - learning rate: 1.2589256584760733e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0739 - accuracy: 0.5234 - learning rate: 1.2915499610244296e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0933 - accuracy: 0.5052 - learning rate: 1.325019638898084e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.1092 - accuracy: 0.4835 - learning rate: 1.359356701868819e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.1046 - accuracy: 0.4853 - learning rate: 1.3945835235062987e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0893 - accuracy: 0.4960 - learning rate: 1.4307232959254179e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0856 - accuracy: 0.4943 - learning rate: 1.4677995750389528e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0765 - accuracy: 0.4910 - learning rate: 1.5058366443554405e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0729 - accuracy: 0.4885 - learning rate: 1.54485951497918e-05\n",
            "9/9 [==============================] - 2s 247ms/step - loss: 1.0729 - accuracy: 0.4885 - val_loss: 0.7850 - val_accuracy: 0.5287\n",
            "Epoch 43/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0025 - accuracy: 0.5469 - learning rate: 1.5848936527618207e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.9714 - accuracy: 0.5703 - learning rate: 1.6259651602013037e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0029 - accuracy: 0.5417 - learning rate: 1.6681009583408013e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0532 - accuracy: 0.5039 - learning rate: 1.7113286958192475e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0555 - accuracy: 0.5031 - learning rate: 1.7556767488713376e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0441 - accuracy: 0.5117 - learning rate: 1.801174039428588e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0465 - accuracy: 0.5033 - learning rate: 1.8478503989172168e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0509 - accuracy: 0.4971 - learning rate: 1.8957362044602633e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0336 - accuracy: 0.5069 - learning rate: 1.9448629245744087e-05\n",
            "9/9 [==============================] - 2s 245ms/step - loss: 1.0336 - accuracy: 0.5069 - val_loss: 0.7873 - val_accuracy: 0.5263\n",
            "Epoch 44/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9833 - accuracy: 0.5312 - learning rate: 1.995262755372096e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0525 - accuracy: 0.4922 - learning rate: 2.0469688024604693e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0499 - accuracy: 0.4948 - learning rate: 2.1000147171434946e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0467 - accuracy: 0.5103 - learning rate: 2.1544352421187796e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0428 - accuracy: 0.5147 - learning rate: 2.2102660295786336e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0500 - accuracy: 0.5108 - learning rate: 2.267543641210068e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0418 - accuracy: 0.5115 - learning rate: 2.326305548194796e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0415 - accuracy: 0.5100 - learning rate: 2.3865903131081723e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0443 - accuracy: 0.5098 - learning rate: 2.448437226121314e-05\n",
            "9/9 [==============================] - 2s 245ms/step - loss: 1.0443 - accuracy: 0.5098 - val_loss: 0.7890 - val_accuracy: 0.5311\n",
            "Epoch 45/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 0.9916 - accuracy: 0.5294 - learning rate: 2.5118868506979197e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0353 - accuracy: 0.5043 - learning rate: 2.576980841695331e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0129 - accuracy: 0.5335 - learning rate: 2.6437615815666504e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0199 - accuracy: 0.5185 - learning rate: 2.7122729079565033e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0326 - accuracy: 0.5049 - learning rate: 2.7825597499031574e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0310 - accuracy: 0.5108 - learning rate: 2.854667945939582e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0292 - accuracy: 0.5138 - learning rate: 2.928644789790269e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0131 - accuracy: 0.5170 - learning rate: 3.0045388484722935e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0165 - accuracy: 0.5142 - learning rate: 3.0823994165984914e-05\n",
            "9/9 [==============================] - 2s 249ms/step - loss: 1.0165 - accuracy: 0.5142 - val_loss: 0.7917 - val_accuracy: 0.5335\n",
            "Epoch 46/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1382 - accuracy: 0.5000 - learning rate: 3.162277789670043e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0987 - accuracy: 0.4766 - learning rate: 3.244226172682829e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0609 - accuracy: 0.5130 - learning rate: 3.328298407723196e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0751 - accuracy: 0.5098 - learning rate: 3.414549064473249e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0845 - accuracy: 0.5047 - learning rate: 3.503034895402379e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0829 - accuracy: 0.4974 - learning rate: 3.5938137443736196e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0742 - accuracy: 0.5011 - learning rate: 3.686945274239406e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0615 - accuracy: 0.5060 - learning rate: 3.782490239245817e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0701 - accuracy: 0.5062 - learning rate: 3.880511212628335e-05\n",
            "9/9 [==============================] - 2s 244ms/step - loss: 1.0701 - accuracy: 0.5062 - val_loss: 0.7941 - val_accuracy: 0.5383\n",
            "Epoch 47/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0741 - accuracy: 0.4453 - learning rate: 3.981072222813964e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0509 - accuracy: 0.4883 - learning rate: 4.084239117219113e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0386 - accuracy: 0.4818 - learning rate: 4.1900795622495934e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0235 - accuracy: 0.4863 - learning rate: 4.2986626795027405e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0363 - accuracy: 0.4812 - learning rate: 4.410059773363173e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0547 - accuracy: 0.4779 - learning rate: 4.524343603407033e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0545 - accuracy: 0.4810 - learning rate: 4.641589111997746e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0474 - accuracy: 0.4912 - learning rate: 4.761873060488142e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0339 - accuracy: 0.5000 - learning rate: 4.8852740292204544e-05\n",
            "9/9 [==============================] - 2s 248ms/step - loss: 1.0339 - accuracy: 0.5000 - val_loss: 0.7953 - val_accuracy: 0.5383\n",
            "Epoch 48/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.8928 - accuracy: 0.5625 - learning rate: 5.0118727813242e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.9989 - accuracy: 0.5156 - learning rate: 5.1417522627161816e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0088 - accuracy: 0.5104 - learning rate: 5.274997602100484e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0141 - accuracy: 0.5144 - learning rate: 5.411695747170597e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0278 - accuracy: 0.5114 - learning rate: 5.5519365560030565e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0031 - accuracy: 0.5310 - learning rate: 5.6958113418659195e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0194 - accuracy: 0.5218 - learning rate: 5.843414692208171e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0182 - accuracy: 0.5261 - learning rate: 5.9948430134681985e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0153 - accuracy: 0.5311 - learning rate: 6.150195258669555e-05\n",
            "9/9 [==============================] - 2s 246ms/step - loss: 1.0153 - accuracy: 0.5311 - val_loss: 0.7970 - val_accuracy: 0.5359\n",
            "Epoch 49/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0245 - accuracy: 0.5859 - learning rate: 6.30957365501672e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0820 - accuracy: 0.5352 - learning rate: 6.473082612501457e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0256 - accuracy: 0.5417 - learning rate: 6.640828360104933e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0108 - accuracy: 0.5226 - learning rate: 6.812921492382884e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0138 - accuracy: 0.5261 - learning rate: 6.989474059082568e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0128 - accuracy: 0.5189 - learning rate: 7.17060174793005e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0227 - accuracy: 0.5207 - learning rate: 7.356423157034442e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0216 - accuracy: 0.5230 - learning rate: 7.547060522483662e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0142 - accuracy: 0.5258 - learning rate: 7.742637535557151e-05\n",
            "9/9 [==============================] - 2s 244ms/step - loss: 1.0142 - accuracy: 0.5258 - val_loss: 0.7978 - val_accuracy: 0.5383\n",
            "Epoch 50/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.1322 - accuracy: 0.4844 - learning rate: 7.94328298070468e-05\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0934 - accuracy: 0.5000 - learning rate: 8.149127825163305e-05\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0826 - accuracy: 0.5000 - learning rate: 8.360307401744649e-05\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0611 - accuracy: 0.5137 - learning rate: 8.57695922604762e-05\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0333 - accuracy: 0.5234 - learning rate: 8.799225906841457e-05\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0211 - accuracy: 0.5326 - learning rate: 9.027252235682681e-05\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0231 - accuracy: 0.5264 - learning rate: 9.261188097298145e-05\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0298 - accuracy: 0.5210 - learning rate: 9.501185559201986e-05\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0360 - accuracy: 0.5178 - learning rate: 9.74740250967443e-05\n",
            "9/9 [==============================] - 2s 248ms/step - loss: 1.0360 - accuracy: 0.5178 - val_loss: 0.7985 - val_accuracy: 0.5383\n",
            "Epoch 51/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0922 - accuracy: 0.4453 - learning rate: 0.00010000000474974513\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0702 - accuracy: 0.4844 - learning rate: 0.00010259143891744316\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0474 - accuracy: 0.4870 - learning rate: 0.0001052500301739201\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0479 - accuracy: 0.4941 - learning rate: 0.00010797751747304574\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0524 - accuracy: 0.4953 - learning rate: 0.00011077568342443556\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0442 - accuracy: 0.5013 - learning rate: 0.00011364636156940833\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0330 - accuracy: 0.5057 - learning rate: 0.00011659143638098612\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0252 - accuracy: 0.5130 - learning rate: 0.00011961282871197909\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0139 - accuracy: 0.5160 - learning rate: 0.00012271251762285829\n",
            "9/9 [==============================] - 2s 255ms/step - loss: 1.0139 - accuracy: 0.5160 - val_loss: 0.8009 - val_accuracy: 0.5478\n",
            "Epoch 52/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 0.8883 - accuracy: 0.5980 - learning rate: 0.00012589253310579807\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0387 - accuracy: 0.5304 - learning rate: 0.0001291549560846761\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0720 - accuracy: 0.5000 - learning rate: 0.00013250192569103092\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0536 - accuracy: 0.4959 - learning rate: 0.00013593562471214682\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0227 - accuracy: 0.5195 - learning rate: 0.00013945830869488418\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0301 - accuracy: 0.5081 - learning rate: 0.0001430722768418491\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0264 - accuracy: 0.5046 - learning rate: 0.00014677990111522377\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0162 - accuracy: 0.5130 - learning rate: 0.00015058361168485135\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0149 - accuracy: 0.5133 - learning rate: 0.0001544858969282359\n",
            "9/9 [==============================] - 2s 256ms/step - loss: 1.0149 - accuracy: 0.5133 - val_loss: 0.8048 - val_accuracy: 0.5502\n",
            "Epoch 53/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9766 - accuracy: 0.5156 - learning rate: 0.00015848930343054235\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0004 - accuracy: 0.5234 - learning rate: 0.00016259645053651184\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.9848 - accuracy: 0.5339 - learning rate: 0.0001668100303504616\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0087 - accuracy: 0.5332 - learning rate: 0.00017113280773628503\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9802 - accuracy: 0.5328 - learning rate: 0.00017556760576553643\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9663 - accuracy: 0.5445 - learning rate: 0.00018011733482126147\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9908 - accuracy: 0.5425 - learning rate: 0.00018478496349416673\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9997 - accuracy: 0.5351 - learning rate: 0.00018957354768645018\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9983 - accuracy: 0.5346 - learning rate: 0.00019448623061180115\n",
            "9/9 [==============================] - 2s 256ms/step - loss: 0.9983 - accuracy: 0.5346 - val_loss: 0.8021 - val_accuracy: 0.5407\n",
            "Epoch 54/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0130 - accuracy: 0.5391 - learning rate: 0.00019952621369156986\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0281 - accuracy: 0.4922 - learning rate: 0.0002046968147624284\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0396 - accuracy: 0.4818 - learning rate: 0.00021000140986870974\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0095 - accuracy: 0.5078 - learning rate: 0.00021544346236623824\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9940 - accuracy: 0.5163 - learning rate: 0.00022102653747424483\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0228 - accuracy: 0.5027 - learning rate: 0.00022675430227536708\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0046 - accuracy: 0.5115 - learning rate: 0.00023263049661181867\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9951 - accuracy: 0.5110 - learning rate: 0.0002386589621892199\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9886 - accuracy: 0.5160 - learning rate: 0.00024484365712851286\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 0.9886 - accuracy: 0.5160 - val_loss: 0.8041 - val_accuracy: 0.5455\n",
            "Epoch 55/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0934 - accuracy: 0.5234 - learning rate: 0.00025118861231021583\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0563 - accuracy: 0.5312 - learning rate: 0.00025769800413399935\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0042 - accuracy: 0.5495 - learning rate: 0.0002643760817591101\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.9998 - accuracy: 0.5430 - learning rate: 0.0002712272107601166\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0047 - accuracy: 0.5359 - learning rate: 0.0002782559022307396\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9844 - accuracy: 0.5378 - learning rate: 0.00028546672547236085\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9824 - accuracy: 0.5402 - learning rate: 0.0002928644244093448\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0079 - accuracy: 0.5332 - learning rate: 0.00030045383027754724\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9960 - accuracy: 0.5435 - learning rate: 0.00030823989072814584\n",
            "9/9 [==============================] - 2s 247ms/step - loss: 0.9960 - accuracy: 0.5435 - val_loss: 0.8035 - val_accuracy: 0.5383\n",
            "Epoch 56/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0679 - accuracy: 0.4531 - learning rate: 0.00031622772803530097\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0870 - accuracy: 0.4478 - learning rate: 0.00032442258088849485\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.0809 - accuracy: 0.4665 - learning rate: 0.00033282977528870106\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.0417 - accuracy: 0.4979 - learning rate: 0.0003414548409637064\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0291 - accuracy: 0.5000 - learning rate: 0.0003503034240566194\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9976 - accuracy: 0.5202 - learning rate: 0.00035938131622970104\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9911 - accuracy: 0.5230 - learning rate: 0.00036869445466436446\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9932 - accuracy: 0.5210 - learning rate: 0.00037824895116500556\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0014 - accuracy: 0.5187 - learning rate: 0.0003880510339513421\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 1.0014 - accuracy: 0.5187 - val_loss: 0.8031 - val_accuracy: 0.5574\n",
            "Epoch 57/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.0389 - accuracy: 0.5000 - learning rate: 0.000398107134969905\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0073 - accuracy: 0.5273 - learning rate: 0.0004084238316863775\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.9775 - accuracy: 0.5469 - learning rate: 0.0004190078761894256\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.9579 - accuracy: 0.5645 - learning rate: 0.0004298661951906979\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9512 - accuracy: 0.5625 - learning rate: 0.0004410059191286564\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9556 - accuracy: 0.5495 - learning rate: 0.00045243429485708475\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9458 - accuracy: 0.5536 - learning rate: 0.00046415883116424084\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9406 - accuracy: 0.5601 - learning rate: 0.0004761872114613652\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9421 - accuracy: 0.5568 - learning rate: 0.0004885272937826812\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 0.9421 - accuracy: 0.5568 - val_loss: 0.8076 - val_accuracy: 0.5526\n",
            "Epoch 58/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9541 - accuracy: 0.5547 - learning rate: 0.0005011871689930558\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.0069 - accuracy: 0.5430 - learning rate: 0.0005141751025803387\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.9902 - accuracy: 0.5495 - learning rate: 0.0005274996510706842\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.9673 - accuracy: 0.5684 - learning rate: 0.0005411694874055684\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9499 - accuracy: 0.5656 - learning rate: 0.0005551935755647719\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9622 - accuracy: 0.5495 - learning rate: 0.0005695810541510582\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9585 - accuracy: 0.5536 - learning rate: 0.0005843414110131562\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9700 - accuracy: 0.5518 - learning rate: 0.0005994842504151165\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9605 - accuracy: 0.5608 - learning rate: 0.0006150195258669555\n",
            "9/9 [==============================] - 2s 251ms/step - loss: 0.9605 - accuracy: 0.5608 - val_loss: 0.7973 - val_accuracy: 0.5574\n",
            "Epoch 59/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.9167 - accuracy: 0.6250 - learning rate: 0.000630957365501672\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.9361 - accuracy: 0.6172 - learning rate: 0.0006473082466982305\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.8809 - accuracy: 0.6250 - learning rate: 0.0006640828214585781\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.8873 - accuracy: 0.6173 - learning rate: 0.0006812920910306275\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9019 - accuracy: 0.6140 - learning rate: 0.0006989473477005959\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9219 - accuracy: 0.6011 - learning rate: 0.0007170601165853441\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9184 - accuracy: 0.6034 - learning rate: 0.0007356422720476985\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9293 - accuracy: 0.5982 - learning rate: 0.00075470597948879\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9304 - accuracy: 0.5924 - learning rate: 0.0007742636953480542\n",
            "9/9 [==============================] - 2s 247ms/step - loss: 0.9304 - accuracy: 0.5924 - val_loss: 0.7975 - val_accuracy: 0.5574\n",
            "Epoch 60/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.8890 - accuracy: 0.5625 - learning rate: 0.0007943282253108919\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.8787 - accuracy: 0.5820 - learning rate: 0.0008149127243086696\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.8831 - accuracy: 0.5885 - learning rate: 0.0008360306965187192\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.9112 - accuracy: 0.5703 - learning rate: 0.0008576958789490163\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9194 - accuracy: 0.5766 - learning rate: 0.0008799225324764848\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9335 - accuracy: 0.5768 - learning rate: 0.000902725150808692\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9216 - accuracy: 0.5828 - learning rate: 0.0009261186933144927\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9237 - accuracy: 0.5842 - learning rate: 0.0009501184686087072\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.5853 - learning rate: 0.0009747401927597821\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 0.9233 - accuracy: 0.5853 - val_loss: 0.7932 - val_accuracy: 0.5478\n",
            "Epoch 61/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 0.8570 - accuracy: 0.6471 - learning rate: 0.0009999999310821295\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.8909 - accuracy: 0.5913 - learning rate: 0.0010259143309667706\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.9185 - accuracy: 0.5587 - learning rate: 0.0010525002144277096\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.9088 - accuracy: 0.5597 - learning rate: 0.0010797750437632203\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9042 - accuracy: 0.5733 - learning rate: 0.0011077567469328642\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.8977 - accuracy: 0.5876 - learning rate: 0.001136463601142168\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9084 - accuracy: 0.5851 - learning rate: 0.001165914349257946\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9056 - accuracy: 0.5872 - learning rate: 0.0011961283162236214\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8979 - accuracy: 0.5906 - learning rate: 0.0012271251762285829\n",
            "9/9 [==============================] - 2s 252ms/step - loss: 0.8979 - accuracy: 0.5906 - val_loss: 0.7699 - val_accuracy: 0.5622\n",
            "Epoch 62/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7518 - accuracy: 0.6719 - learning rate: 0.0012589253019541502\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7969 - accuracy: 0.6478 - learning rate: 0.0012915495317429304\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7746 - accuracy: 0.6732 - learning rate: 0.0013250191695988178\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7900 - accuracy: 0.6626 - learning rate: 0.0013593562180176377\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.8042 - accuracy: 0.6417 - learning rate: 0.001394583028741181\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.8154 - accuracy: 0.6321 - learning rate: 0.0014307227684184909\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.8377 - accuracy: 0.6218 - learning rate: 0.0014677990693598986\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.8529 - accuracy: 0.6132 - learning rate: 0.001505836145952344\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8514 - accuracy: 0.6137 - learning rate: 0.001544858911074698\n",
            "9/9 [==============================] - 2s 243ms/step - loss: 0.8514 - accuracy: 0.6137 - val_loss: 0.7385 - val_accuracy: 0.5885\n",
            "Epoch 63/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7334 - accuracy: 0.7109 - learning rate: 0.0015848929760977626\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.8176 - accuracy: 0.6391 - learning rate: 0.001625964418053627\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.8181 - accuracy: 0.6229 - learning rate: 0.0016681002452969551\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.8048 - accuracy: 0.6379 - learning rate: 0.0017113280482590199\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.8379 - accuracy: 0.6368 - learning rate: 0.0017556759994477034\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.8290 - accuracy: 0.6429 - learning rate: 0.0018011732026934624\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.8210 - accuracy: 0.6506 - learning rate: 0.0018478494603186846\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.8236 - accuracy: 0.6483 - learning rate: 0.0018957352731376886\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8181 - accuracy: 0.6501 - learning rate: 0.0019448620732873678\n",
            "9/9 [==============================] - 2s 239ms/step - loss: 0.8181 - accuracy: 0.6501 - val_loss: 0.6949 - val_accuracy: 0.6172\n",
            "Epoch 64/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7255 - accuracy: 0.6562 - learning rate: 0.0019952619913965464\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7680 - accuracy: 0.6367 - learning rate: 0.0020469678565859795\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7959 - accuracy: 0.6250 - learning rate: 0.0021000136621296406\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.8137 - accuracy: 0.6172 - learning rate: 0.002154434099793434\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.8255 - accuracy: 0.6219 - learning rate: 0.002210264792665839\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.8051 - accuracy: 0.6321 - learning rate: 0.0022675422951579094\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.8052 - accuracy: 0.6391 - learning rate: 0.0023263043258339167\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.8162 - accuracy: 0.6333 - learning rate: 0.0023865890689194202\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8351 - accuracy: 0.6252 - learning rate: 0.0024484361056238413\n",
            "9/9 [==============================] - 2s 242ms/step - loss: 0.8351 - accuracy: 0.6252 - val_loss: 0.7452 - val_accuracy: 0.5789\n",
            "Epoch 65/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 0.8536 - accuracy: 0.6176 - learning rate: 0.002511885715648532\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.8414 - accuracy: 0.6304 - learning rate: 0.002576979575678706\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.8335 - accuracy: 0.6397 - learning rate: 0.0026437602937221527\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.8107 - accuracy: 0.6605 - learning rate: 0.0027122716419398785\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.8001 - accuracy: 0.6580 - learning rate: 0.0027825585566461086\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.8139 - accuracy: 0.6496 - learning rate: 0.002854666905477643\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.8024 - accuracy: 0.6563 - learning rate: 0.0029286437202244997\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.7975 - accuracy: 0.6513 - learning rate: 0.0030045376624912024\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7859 - accuracy: 0.6563 - learning rate: 0.0030823983252048492\n",
            "9/9 [==============================] - 2s 253ms/step - loss: 0.7859 - accuracy: 0.6563 - val_loss: 0.7115 - val_accuracy: 0.5981\n",
            "Epoch 66/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 0.9610 - accuracy: 0.5980 - learning rate: 0.0031622766982764006\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.8621 - accuracy: 0.6261 - learning rate: 0.0032442251686006784\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.8384 - accuracy: 0.6397 - learning rate: 0.0033282972872257233\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.8619 - accuracy: 0.6296 - learning rate: 0.0034145480021834373\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.8353 - accuracy: 0.6433 - learning rate: 0.0035030338913202286\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.8317 - accuracy: 0.6509 - learning rate: 0.003593812696635723\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.8104 - accuracy: 0.6586 - learning rate: 0.0036869440227746964\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.8169 - accuracy: 0.6573 - learning rate: 0.0037824888713657856\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8039 - accuracy: 0.6634 - learning rate: 0.00388050964102149\n",
            "9/9 [==============================] - 2s 253ms/step - loss: 0.8039 - accuracy: 0.6634 - val_loss: 0.6741 - val_accuracy: 0.6651\n",
            "Epoch 67/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.6508 - accuracy: 0.7188 - learning rate: 0.003981070592999458\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7086 - accuracy: 0.6836 - learning rate: 0.004084237385541201\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7380 - accuracy: 0.6693 - learning rate: 0.004190078005194664\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7614 - accuracy: 0.6605 - learning rate: 0.00429866136983037\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.7584 - accuracy: 0.6710 - learning rate: 0.004410058259963989\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.7642 - accuracy: 0.6698 - learning rate: 0.0045243422500789165\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.7750 - accuracy: 0.6667 - learning rate: 0.004641587845981121\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.7788 - accuracy: 0.6623 - learning rate: 0.004761871881783009\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7653 - accuracy: 0.6679 - learning rate: 0.004885272588580847\n",
            "9/9 [==============================] - 2s 249ms/step - loss: 0.7653 - accuracy: 0.6679 - val_loss: 0.7063 - val_accuracy: 0.6411\n",
            "Epoch 68/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.6767 - accuracy: 0.7188 - learning rate: 0.0050118714570999146\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6918 - accuracy: 0.7031 - learning rate: 0.005141750909388065\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7012 - accuracy: 0.6953 - learning rate: 0.0052749961614608765\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7078 - accuracy: 0.6973 - learning rate: 0.0054116942919790745\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6961 - accuracy: 0.7016 - learning rate: 0.0055519347079098225\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.6977 - accuracy: 0.7044 - learning rate: 0.005695809610188007\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6969 - accuracy: 0.6987 - learning rate: 0.005843413062393665\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6882 - accuracy: 0.7064 - learning rate: 0.005994841456413269\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.6998 - learning rate: 0.006150193978101015\n",
            "9/9 [==============================] - 2s 249ms/step - loss: 0.6991 - accuracy: 0.6998 - val_loss: 0.6539 - val_accuracy: 0.6794\n",
            "Epoch 69/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7063 - accuracy: 0.7656 - learning rate: 0.0063095721416175365\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7740 - accuracy: 0.7261 - learning rate: 0.006473080720752478\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7804 - accuracy: 0.7123 - learning rate: 0.006640826351940632\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7915 - accuracy: 0.6914 - learning rate: 0.006812918931245804\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.7752 - accuracy: 0.6873 - learning rate: 0.006989471614360809\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.7725 - accuracy: 0.6873 - learning rate: 0.007170599419623613\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.7590 - accuracy: 0.6931 - learning rate: 0.007356421090662479\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.7530 - accuracy: 0.6974 - learning rate: 0.007547058165073395\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7448 - accuracy: 0.7060 - learning rate: 0.007742635440081358\n",
            "9/9 [==============================] - 2s 246ms/step - loss: 0.7448 - accuracy: 0.7060 - val_loss: 0.6314 - val_accuracy: 0.6890\n",
            "Epoch 70/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7022 - accuracy: 0.6953 - learning rate: 0.007943280972540379\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7140 - accuracy: 0.7148 - learning rate: 0.00814912561327219\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7305 - accuracy: 0.6979 - learning rate: 0.008360304869711399\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7251 - accuracy: 0.6934 - learning rate: 0.008576956577599049\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.7281 - accuracy: 0.6873 - learning rate: 0.008799223229289055\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.7145 - accuracy: 0.7008 - learning rate: 0.009027249179780483\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6982 - accuracy: 0.7115 - learning rate: 0.009261184372007847\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6870 - accuracy: 0.7144 - learning rate: 0.009501182474195957\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.7123 - learning rate: 0.00974739994853735\n",
            "9/9 [==============================] - 2s 249ms/step - loss: 0.6929 - accuracy: 0.7123 - val_loss: 0.6199 - val_accuracy: 0.7512\n",
            "Epoch 71/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.5768 - accuracy: 0.7656 - learning rate: 0.009999997913837433\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6908 - accuracy: 0.7383 - learning rate: 0.010259141214191914\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.6660 - accuracy: 0.7344 - learning rate: 0.010525000281631947\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7081 - accuracy: 0.7188 - learning rate: 0.010797749273478985\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6988 - accuracy: 0.7231 - learning rate: 0.01107756607234478\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.7053 - accuracy: 0.7170 - learning rate: 0.011364634148776531\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6837 - accuracy: 0.7207 - learning rate: 0.011659141629934311\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6806 - accuracy: 0.7204 - learning rate: 0.011961281299591064\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.7220 - learning rate: 0.01227125059813261\n",
            "9/9 [==============================] - 2s 253ms/step - loss: 0.6782 - accuracy: 0.7220 - val_loss: 0.6053 - val_accuracy: 0.7536\n",
            "Epoch 72/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7128 - accuracy: 0.6641 - learning rate: 0.012589252553880215\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7043 - accuracy: 0.6680 - learning rate: 0.012915494851768017\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7079 - accuracy: 0.6823 - learning rate: 0.013250191695988178\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7085 - accuracy: 0.6875 - learning rate: 0.013593561947345734\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6596 - accuracy: 0.7109 - learning rate: 0.013945830054581165\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.6849 - accuracy: 0.7070 - learning rate: 0.014307226985692978\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6763 - accuracy: 0.7065 - learning rate: 0.014677989296615124\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6714 - accuracy: 0.7080 - learning rate: 0.015058360062539577\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.7087 - learning rate: 0.015448587946593761\n",
            "9/9 [==============================] - 2s 251ms/step - loss: 0.6696 - accuracy: 0.7087 - val_loss: 0.6807 - val_accuracy: 0.7321\n",
            "Epoch 73/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7621 - accuracy: 0.6953 - learning rate: 0.015848929062485695\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6573 - accuracy: 0.7422 - learning rate: 0.01625964418053627\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.6432 - accuracy: 0.7422 - learning rate: 0.01668100245296955\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.6707 - accuracy: 0.7207 - learning rate: 0.017113279551267624\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6598 - accuracy: 0.7248 - learning rate: 0.01755675859749317\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.6606 - accuracy: 0.7183 - learning rate: 0.018011730164289474\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6469 - accuracy: 0.7253 - learning rate: 0.01847849227488041\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6249 - accuracy: 0.7325 - learning rate: 0.01895735040307045\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6253 - accuracy: 0.7327 - learning rate: 0.019448617473244667\n",
            "9/9 [==============================] - 2s 252ms/step - loss: 0.6253 - accuracy: 0.7327 - val_loss: 0.7827 - val_accuracy: 0.6866\n",
            "Epoch 74/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.5544 - accuracy: 0.7734 - learning rate: 0.019952615723013878\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6152 - accuracy: 0.7617 - learning rate: 0.020469674840569496\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.6159 - accuracy: 0.7448 - learning rate: 0.021000133827328682\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.6027 - accuracy: 0.7559 - learning rate: 0.021544339135289192\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.5830 - accuracy: 0.7656 - learning rate: 0.02210264652967453\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5764 - accuracy: 0.7722 - learning rate: 0.022675422951579094\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5902 - accuracy: 0.7667 - learning rate: 0.02326304279267788\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5954 - accuracy: 0.7555 - learning rate: 0.023865889757871628\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6052 - accuracy: 0.7522 - learning rate: 0.024484358727931976\n",
            "9/9 [==============================] - 2s 255ms/step - loss: 0.6052 - accuracy: 0.7522 - val_loss: 0.6064 - val_accuracy: 0.7632\n",
            "Epoch 75/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.5474 - accuracy: 0.7891 - learning rate: 0.025118855759501457\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6103 - accuracy: 0.7500 - learning rate: 0.0257697943598032\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.6395 - accuracy: 0.7526 - learning rate: 0.026437602937221527\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.6373 - accuracy: 0.7441 - learning rate: 0.02712271735072136\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6352 - accuracy: 0.7453 - learning rate: 0.02782558463513851\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.6273 - accuracy: 0.7526 - learning rate: 0.028546666726469994\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6149 - accuracy: 0.7556 - learning rate: 0.02928643487393856\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6043 - accuracy: 0.7555 - learning rate: 0.030045373365283012\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6048 - accuracy: 0.7558 - learning rate: 0.030823979526758194\n",
            "9/9 [==============================] - 2s 255ms/step - loss: 0.6048 - accuracy: 0.7558 - val_loss: 0.9829 - val_accuracy: 0.5957\n",
            "Epoch 76/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.5240 - accuracy: 0.7578 - learning rate: 0.031622763723134995\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.5491 - accuracy: 0.7773 - learning rate: 0.03244224935770035\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.6056 - accuracy: 0.7552 - learning rate: 0.033282969146966934\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.6185 - accuracy: 0.7441 - learning rate: 0.034145474433898926\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6129 - accuracy: 0.7437 - learning rate: 0.03503033146262169\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5915 - accuracy: 0.7591 - learning rate: 0.03593812137842178\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6007 - accuracy: 0.7621 - learning rate: 0.036869436502456665\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6054 - accuracy: 0.7625 - learning rate: 0.03782488405704498\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6043 - accuracy: 0.7629 - learning rate: 0.03880509361624718\n",
            "9/9 [==============================] - 2s 257ms/step - loss: 0.6043 - accuracy: 0.7629 - val_loss: 2.7371 - val_accuracy: 0.5359\n",
            "Epoch 77/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.6111 - accuracy: 0.7734 - learning rate: 0.039810702204704285\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.5790 - accuracy: 0.7930 - learning rate: 0.04084237292408943\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.5973 - accuracy: 0.7630 - learning rate: 0.04190077632665634\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.6318 - accuracy: 0.7559 - learning rate: 0.042986609041690826\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6131 - accuracy: 0.7590 - learning rate: 0.044100578874349594\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.6045 - accuracy: 0.7601 - learning rate: 0.04524341598153114\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6005 - accuracy: 0.7552 - learning rate: 0.046415869146585464\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5999 - accuracy: 0.7525 - learning rate: 0.04761870577931404\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5926 - accuracy: 0.7575 - learning rate: 0.04885271564126015\n",
            "9/9 [==============================] - 2s 255ms/step - loss: 0.5926 - accuracy: 0.7575 - val_loss: 2.1486 - val_accuracy: 0.5526\n",
            "Epoch 78/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.4894 - accuracy: 0.7891 - learning rate: 0.05011870339512825\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.5481 - accuracy: 0.7656 - learning rate: 0.05141749605536461\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.5474 - accuracy: 0.7786 - learning rate: 0.05274994671344757\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.5668 - accuracy: 0.7716 - learning rate: 0.054116927087306976\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6014 - accuracy: 0.7508 - learning rate: 0.055519331246614456\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5949 - accuracy: 0.7547 - learning rate: 0.05695807933807373\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5895 - accuracy: 0.7540 - learning rate: 0.05843411013484001\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5869 - accuracy: 0.7565 - learning rate: 0.0599483922123909\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.7602 - learning rate: 0.061501916497945786\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 0.5760 - accuracy: 0.7602 - val_loss: 1.2834 - val_accuracy: 0.6077\n",
            "Epoch 79/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.4793 - accuracy: 0.7812 - learning rate: 0.06309569627046585\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.5148 - accuracy: 0.7891 - learning rate: 0.06473077833652496\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.4733 - accuracy: 0.8047 - learning rate: 0.06640823185443878\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.5090 - accuracy: 0.7871 - learning rate: 0.06812915951013565\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.5267 - accuracy: 0.7828 - learning rate: 0.0698946863412857\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5297 - accuracy: 0.7786 - learning rate: 0.07170595973730087\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5265 - accuracy: 0.7779 - learning rate: 0.07356417179107666\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5389 - accuracy: 0.7725 - learning rate: 0.07547053694725037\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5418 - accuracy: 0.7709 - learning rate: 0.07742630690336227\n",
            "9/9 [==============================] - 2s 251ms/step - loss: 0.5418 - accuracy: 0.7709 - val_loss: 3.3007 - val_accuracy: 0.5718\n",
            "Epoch 80/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.5783 - accuracy: 0.7422 - learning rate: 0.07943276315927505\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6288 - accuracy: 0.7109 - learning rate: 0.08149120956659317\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.6302 - accuracy: 0.7031 - learning rate: 0.08360300213098526\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.6210 - accuracy: 0.7070 - learning rate: 0.08576951920986176\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.5879 - accuracy: 0.7344 - learning rate: 0.08799218386411667\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5704 - accuracy: 0.7513 - learning rate: 0.0902724489569664\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5642 - accuracy: 0.7522 - learning rate: 0.09261180460453033\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5762 - accuracy: 0.7455 - learning rate: 0.09501177817583084\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5712 - accuracy: 0.7433 - learning rate: 0.09747394919395447\n",
            "9/9 [==============================] - 2s 248ms/step - loss: 0.5712 - accuracy: 0.7433 - val_loss: 5.0933 - val_accuracy: 0.4833\n",
            "Epoch 81/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.5290 - accuracy: 0.7188 - learning rate: 0.09999992698431015\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.5165 - accuracy: 0.7500 - learning rate: 0.10259135812520981\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.5080 - accuracy: 0.7630 - learning rate: 0.10524994879961014\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.5209 - accuracy: 0.7637 - learning rate: 0.10797743499279022\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.5227 - accuracy: 0.7625 - learning rate: 0.11077560484409332\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5439 - accuracy: 0.7565 - learning rate: 0.11364628374576569\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5558 - accuracy: 0.7471 - learning rate: 0.11659135669469833\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5550 - accuracy: 0.7535 - learning rate: 0.11961274594068527\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7522 - learning rate: 0.12271243333816528\n",
            "9/9 [==============================] - 2s 245ms/step - loss: 0.5664 - accuracy: 0.7522 - val_loss: 1.8849 - val_accuracy: 0.6890\n",
            "Epoch 82/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.8208 - accuracy: 0.6562 - learning rate: 0.12589244544506073\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7328 - accuracy: 0.6797 - learning rate: 0.12915486097335815\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.6647 - accuracy: 0.7109 - learning rate: 0.13250182569026947\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.6323 - accuracy: 0.7129 - learning rate: 0.13593553006649017\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.6047 - accuracy: 0.7264 - learning rate: 0.13945820927619934\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5984 - accuracy: 0.7251 - learning rate: 0.14307217299938202\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6147 - accuracy: 0.7184 - learning rate: 0.14677979052066803\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6090 - accuracy: 0.7244 - learning rate: 0.15058349072933197\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.7282 - learning rate: 0.1544857621192932\n",
            "9/9 [==============================] - 2s 249ms/step - loss: 0.5980 - accuracy: 0.7282 - val_loss: 2.6048 - val_accuracy: 0.6148\n",
            "Epoch 83/100\n",
            "1/9 [==>...........................] - ETA: 1s - loss: 0.6271 - accuracy: 0.7255 - learning rate: 0.1584891676902771\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6180 - accuracy: 0.7087 - learning rate: 0.16259631514549255\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.5837 - accuracy: 0.7402 - learning rate: 0.16680990159511566\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.5807 - accuracy: 0.7407 - learning rate: 0.1711326688528061\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.5772 - accuracy: 0.7492 - learning rate: 0.17556746304035187\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5684 - accuracy: 0.7547 - learning rate: 0.1801171898841858\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5819 - accuracy: 0.7448 - learning rate: 0.18478481471538544\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5883 - accuracy: 0.7435 - learning rate: 0.18957339227199554\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5849 - accuracy: 0.7460 - learning rate: 0.19448606669902802\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 0.5849 - accuracy: 0.7460 - val_loss: 4.2608 - val_accuracy: 0.5789\n",
            "Epoch 84/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.6941 - accuracy: 0.7109 - learning rate: 0.19952605664730072\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.6135 - accuracy: 0.7344 - learning rate: 0.2046966403722763\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.5754 - accuracy: 0.7344 - learning rate: 0.21000123023986816\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.5720 - accuracy: 0.7324 - learning rate: 0.21544328331947327\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.5720 - accuracy: 0.7313 - learning rate: 0.22102636098861694\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5574 - accuracy: 0.7358 - learning rate: 0.2267541140317917\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5519 - accuracy: 0.7356 - learning rate: 0.23263029754161835\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5547 - accuracy: 0.7385 - learning rate: 0.23865877091884613\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5769 - accuracy: 0.7345 - learning rate: 0.2448434680700302\n",
            "9/9 [==============================] - 2s 242ms/step - loss: 0.5769 - accuracy: 0.7345 - val_loss: 2.4105 - val_accuracy: 0.6029\n",
            "Epoch 85/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.3909 - accuracy: 0.8359 - learning rate: 0.2511884272098541\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.4994 - accuracy: 0.7812 - learning rate: 0.25769782066345215\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.5831 - accuracy: 0.7318 - learning rate: 0.2643758952617645\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.5796 - accuracy: 0.7402 - learning rate: 0.27122703194618225\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.5610 - accuracy: 0.7410 - learning rate: 0.2782557010650635\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.5282 - accuracy: 0.7642 - learning rate: 0.2854665219783783\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5243 - accuracy: 0.7667 - learning rate: 0.29286420345306396\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5357 - accuracy: 0.7585 - learning rate: 0.3004536032676697\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.7513 - learning rate: 0.3082396686077118\n",
            "9/9 [==============================] - 2s 248ms/step - loss: 0.5669 - accuracy: 0.7513 - val_loss: 2.1552 - val_accuracy: 0.6483\n",
            "Epoch 86/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.8395 - accuracy: 0.6172 - learning rate: 0.3162274956703186\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7379 - accuracy: 0.6680 - learning rate: 0.32442232966423035\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7493 - accuracy: 0.6823 - learning rate: 0.3328295350074768\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7885 - accuracy: 0.6562 - learning rate: 0.3414545953273773\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9903 - accuracy: 0.6484 - learning rate: 0.35030317306518555\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9826 - accuracy: 0.6263 - learning rate: 0.3593810498714447\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9647 - accuracy: 0.6368 - learning rate: 0.3686941862106323\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9398 - accuracy: 0.6403 - learning rate: 0.3782486617565155\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9216 - accuracy: 0.6474 - learning rate: 0.38805073499679565\n",
            "9/9 [==============================] - 2s 248ms/step - loss: 0.9216 - accuracy: 0.6474 - val_loss: 55.3376 - val_accuracy: 0.4665\n",
            "Epoch 87/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.7623 - accuracy: 0.6328 - learning rate: 0.39810681343078613\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7719 - accuracy: 0.6562 - learning rate: 0.408423513174057\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7997 - accuracy: 0.6589 - learning rate: 0.4190075397491455\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.9251 - accuracy: 0.6191 - learning rate: 0.42986586689949036\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.0969 - accuracy: 0.6109 - learning rate: 0.44100555777549744\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.0896 - accuracy: 0.6068 - learning rate: 0.4524339437484741\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.1144 - accuracy: 0.5815 - learning rate: 0.46415847539901733\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.0883 - accuracy: 0.5902 - learning rate: 0.4761868417263031\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0723 - accuracy: 0.5941 - learning rate: 0.4885269105434418\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 1.0723 - accuracy: 0.5941 - val_loss: 83.4626 - val_accuracy: 0.4665\n",
            "Epoch 88/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.8022 - accuracy: 0.7031 - learning rate: 0.5011867880821228\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.7258 - accuracy: 0.7305 - learning rate: 0.5141746997833252\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.7441 - accuracy: 0.7005 - learning rate: 0.5274991989135742\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7664 - accuracy: 0.6953 - learning rate: 0.5411689877510071\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.8770 - accuracy: 0.6797 - learning rate: 0.5551930665969849\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9885 - accuracy: 0.6654 - learning rate: 0.5695805549621582\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.0151 - accuracy: 0.6417 - learning rate: 0.5843408703804016\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.1728 - accuracy: 0.6416 - learning rate: 0.5994836688041687\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1251 - accuracy: 0.6458 - learning rate: 0.615018904209137\n",
            "9/9 [==============================] - 2s 256ms/step - loss: 1.1251 - accuracy: 0.6458 - val_loss: 111.2192 - val_accuracy: 0.6483\n",
            "Epoch 89/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.2124 - accuracy: 0.6484 - learning rate: 0.6309567093849182\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.5124 - accuracy: 0.6055 - learning rate: 0.6473075747489929\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.3027 - accuracy: 0.6042 - learning rate: 0.6640821695327759\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.4888 - accuracy: 0.6133 - learning rate: 0.6812914609909058\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.5151 - accuracy: 0.5984 - learning rate: 0.6989467144012451\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.5920 - accuracy: 0.5833 - learning rate: 0.7170594930648804\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.6329 - accuracy: 0.5690 - learning rate: 0.7356416583061218\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.6818 - accuracy: 0.5852 - learning rate: 0.7547053694725037\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.7175 - accuracy: 0.5933 - learning rate: 0.7742630839347839\n",
            "9/9 [==============================] - 2s 245ms/step - loss: 1.7175 - accuracy: 0.5933 - val_loss: 1083.1954 - val_accuracy: 0.5574\n",
            "Epoch 90/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 2.1828 - accuracy: 0.4375 - learning rate: 0.7943276166915894\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 3.4131 - accuracy: 0.5469 - learning rate: 0.8149121403694153\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 3.0057 - accuracy: 0.5495 - learning rate: 0.8360300660133362\n",
            "4/9 [============>.................] - ETA: 1s - loss: 2.9644 - accuracy: 0.5508 - learning rate: 0.8576952815055847\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 2.5806 - accuracy: 0.5516 - learning rate: 0.8799219131469727\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 2.6400 - accuracy: 0.5485 - learning rate: 0.9027245044708252\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 2.4396 - accuracy: 0.5494 - learning rate: 0.926118016242981\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 2.2501 - accuracy: 0.5631 - learning rate: 0.9501177668571472\n",
            "9/9 [==============================] - ETA: 0s - loss: 2.1971 - accuracy: 0.5488 - learning rate: 0.9747394919395447\n",
            "9/9 [==============================] - 2s 257ms/step - loss: 2.1971 - accuracy: 0.5488 - val_loss: 6743.7456 - val_accuracy: 0.4689\n",
            "Epoch 91/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 3.8306 - accuracy: 0.5938 - learning rate: 0.9999992251396179\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 2.9860 - accuracy: 0.6289 - learning rate: 1.0259135961532593\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 2.7991 - accuracy: 0.5859 - learning rate: 1.052499532699585\n",
            "4/9 [============>.................] - ETA: 1s - loss: 2.6717 - accuracy: 0.5926 - learning rate: 1.0797743797302246\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 2.7086 - accuracy: 0.5717 - learning rate: 1.1077560186386108\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 2.9674 - accuracy: 0.5350 - learning rate: 1.1364628076553345\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 3.2723 - accuracy: 0.5437 - learning rate: 1.165913462638855\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 3.1920 - accuracy: 0.5371 - learning rate: 1.1961274147033691\n",
            "9/9 [==============================] - ETA: 0s - loss: 3.1465 - accuracy: 0.5311 - learning rate: 1.2271243333816528\n",
            "9/9 [==============================] - 2s 248ms/step - loss: 3.1465 - accuracy: 0.5311 - val_loss: 25647.8398 - val_accuracy: 0.4689\n",
            "Epoch 92/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 3.3936 - accuracy: 0.5312 - learning rate: 1.2589244842529297\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 2.4283 - accuracy: 0.5312 - learning rate: 1.291548728942871\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 2.0580 - accuracy: 0.5677 - learning rate: 1.3250184059143066\n",
            "4/9 [============>.................] - ETA: 1s - loss: 2.0070 - accuracy: 0.5508 - learning rate: 1.3593554496765137\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 1.8134 - accuracy: 0.5594 - learning rate: 1.3945822715759277\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.6550 - accuracy: 0.5742 - learning rate: 1.4307219982147217\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.6059 - accuracy: 0.5580 - learning rate: 1.4677982330322266\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.6406 - accuracy: 0.5621 - learning rate: 1.5058352947235107\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5711 - accuracy: 0.5515 - learning rate: 1.5448580980300903\n",
            "9/9 [==============================] - 2s 254ms/step - loss: 1.5711 - accuracy: 0.5515 - val_loss: 3583.3879 - val_accuracy: 0.4689\n",
            "Epoch 93/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 0.8857 - accuracy: 0.6719 - learning rate: 1.5848921537399292\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 0.8874 - accuracy: 0.6328 - learning rate: 1.6259636878967285\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 0.8209 - accuracy: 0.6198 - learning rate: 1.6680995225906372\n",
            "4/9 [============>.................] - ETA: 1s - loss: 0.7905 - accuracy: 0.6172 - learning rate: 1.711327314376831\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.9038 - accuracy: 0.6094 - learning rate: 1.7556753158569336\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.9400 - accuracy: 0.5984 - learning rate: 1.8011724948883057\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.9564 - accuracy: 0.6126 - learning rate: 1.8478487730026245\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.9401 - accuracy: 0.5962 - learning rate: 1.8957345485687256\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9229 - accuracy: 0.6021 - learning rate: 1.9448612928390503\n",
            "9/9 [==============================] - 2s 251ms/step - loss: 0.9229 - accuracy: 0.6021 - val_loss: 108.7784 - val_accuracy: 0.6268\n",
            "Epoch 94/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.3544 - accuracy: 0.5234 - learning rate: 1.9952611923217773\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.5855 - accuracy: 0.5217 - learning rate: 2.046967029571533\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.6193 - accuracy: 0.4944 - learning rate: 2.10001277923584\n",
            "4/9 [============>.................] - ETA: 1s - loss: 1.8468 - accuracy: 0.5267 - learning rate: 2.154433250427246\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 2.3388 - accuracy: 0.5195 - learning rate: 2.210263967514038\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 2.1213 - accuracy: 0.5121 - learning rate: 2.2675416469573975\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 1.9732 - accuracy: 0.5287 - learning rate: 2.326303482055664\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.9147 - accuracy: 0.5210 - learning rate: 2.3865880966186523\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.9707 - accuracy: 0.5213 - learning rate: 2.448435068130493\n",
            "9/9 [==============================] - 2s 256ms/step - loss: 1.9707 - accuracy: 0.5213 - val_loss: 546.2747 - val_accuracy: 0.4904\n",
            "Epoch 95/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 1.4963 - accuracy: 0.6484 - learning rate: 2.5118846893310547\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 1.6708 - accuracy: 0.5781 - learning rate: 2.5769786834716797\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 1.9131 - accuracy: 0.5833 - learning rate: 2.6437594890594482\n",
            "4/9 [============>.................] - ETA: 1s - loss: 2.6582 - accuracy: 0.5352 - learning rate: 2.712270736694336\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 3.3100 - accuracy: 0.5297 - learning rate: 2.782557487487793\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 3.3978 - accuracy: 0.5508 - learning rate: 2.854665756225586\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 3.8132 - accuracy: 0.5335 - learning rate: 2.928642511367798\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 3.5646 - accuracy: 0.5527 - learning rate: 3.0045363903045654\n",
            "9/9 [==============================] - ETA: 0s - loss: 3.6480 - accuracy: 0.5417 - learning rate: 3.082396984100342\n",
            "9/9 [==============================] - 2s 257ms/step - loss: 3.6480 - accuracy: 0.5417 - val_loss: 6919.9673 - val_accuracy: 0.4689\n",
            "Epoch 96/100\n",
            "1/9 [==>...........................] - ETA: 2s - loss: 3.4299 - accuracy: 0.5938 - learning rate: 3.1622753143310547\n",
            "2/9 [=====>........................] - ETA: 1s - loss: 2.4772 - accuracy: 0.5156 - learning rate: 3.2442235946655273\n",
            "3/9 [=========>....................] - ETA: 1s - loss: 4.0311 - accuracy: 0.5260 - learning rate: 3.3282957077026367\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 4.0311 - accuracy: 0.5260 - val_loss: 1600.4427 - val_accuracy: 0.4833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9_RytfbFvsI"
      },
      "source": [
        "##**Evaluate the Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4raNKDSGOjT"
      },
      "source": [
        "##**Save the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "LEmW9ES6Itv2",
        "outputId": "d007ab39-e45c-4026-f3c3-fbe6cb348d27"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import math \n",
        "from scipy import signal\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add traces\n",
        "fig.add_trace(go.Scatter(x=lr_screening.lr_list, y=signal.savgol_filter(lr_screening.loss_list,53,3),mode='lines',name='Train Loss'))\n",
        "fig.update_xaxes(type=\"log\")\n",
        "fig.update_layout( yaxis_title=\"Loss\",xaxis_title=\"Learning Rate - Log Scale\")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"3e8dc70b-8d19-44c9-93ea-2cf71af7d4e7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"3e8dc70b-8d19-44c9-93ea-2cf71af7d4e7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '3e8dc70b-8d19-44c9-93ea-2cf71af7d4e7',\n",
              "                        [{\"mode\": \"lines\", \"name\": \"Train Loss\", \"type\": \"scatter\", \"x\": [9.999999717180685e-10, 1.025914353469659e-09, 1.0525003091288454e-09, 1.0797751581748116e-09, 1.1077568862205567e-09, 1.1364637009236844e-09, 1.1659144760756135e-09, 1.1961284185346699e-09, 1.227125290270692e-09, 1.2589254083650303e-09, 1.291549645010548e-09, 1.3250193164893176e-09, 1.3593564052172269e-09, 1.3945832266770708e-09, 1.430722984530064e-09, 1.4677992155043285e-09, 1.505836344506406e-09, 1.5448591295097458e-09, 1.5848932166662166e-09, 1.6259646962168972e-09, 1.668100546581286e-09, 1.7113283012903935e-09, 1.7556762710313478e-09, 1.8011735436473941e-09, 1.8478497620932899e-09, 1.8957355685245147e-09, 1.944862271230363e-09, 1.9952621777008517e-09, 2.0469681505375092e-09, 2.1000139405202845e-09, 2.1544344086521505e-09, 2.2102653041145004e-09, 2.267542820177937e-09, 2.326304704425297e-09, 2.3865893705732333e-09, 2.448436342561422e-09, 2.5118860325079595e-09, 2.576979962753967e-09, 2.6437607658635898e-09, 2.7122721846239983e-09, 2.7825590720453874e-09, 2.8546673913609766e-09, 2.92864421602701e-09, 3.004538173811966e-09, 3.082398780662743e-09, 3.1622771068384736e-09, 3.2442255548659205e-09, 3.3282976374948703e-09, 3.4145484217873445e-09, 3.503034307072994e-09, 3.5938132469937045e-09, 3.6869445274589907e-09, 3.782489432779812e-09, 3.880510135445547e-09, 3.981071028391625e-09, 4.084237836821103e-09, 4.190078062293878e-09, 4.298661426815897e-09, 4.4100585405715265e-09, 4.524342234191181e-09, 4.6415875587513256e-09, 4.761871341685264e-09, 4.8852721867831406e-09, 5.011870918281147e-09, 5.141750580861526e-09, 5.2749959955633585e-09, 5.411694203871775e-09, 5.551934911807166e-09, 5.69580960174676e-09, 5.8434128646922545e-09, 5.994841068002188e-09, 6.1501936876595664e-09, 6.309571976004236e-09, 6.47308073808972e-09, 6.6408265553263846e-09, 6.812919561838271e-09, 6.989472112195472e-09, 7.170599669592548e-09, 7.356421249937739e-09, 7.547058089585335e-09, 7.742634977603302e-09, 7.943280699862498e-09, 8.149125818590619e-09, 8.360305336907459e-09, 8.576956922468071e-09, 8.799223571998027e-09, 9.027250058579739e-09, 9.261185596187715e-09, 9.501182951510145e-09, 9.747400220305735e-09, 9.999998162868451e-09, 1.0259141980384356e-08, 1.0525001314931615e-08, 1.0797750249480487e-08, 1.1077567307893332e-08, 1.136463545492461e-08, 1.1659142984399296e-08, 1.1961282631034464e-08, 1.227125157043929e-08, 1.2589253195471883e-08, 1.2915495339882455e-08, 1.3250192054670151e-08, 1.359356271990464e-08, 1.3945831156547683e-08, 1.4307228290988405e-08, 1.4677991266864865e-08, 1.505836166870722e-08, 1.544858996282983e-08, 1.5848931056439142e-08, 1.6259646073990552e-08, 1.668100502172365e-08, 1.711328323494854e-08, 1.7556763154402688e-08, 1.8011736102607756e-08, 1.8478498731155923e-08, 1.8957356573423567e-08, 1.944862404457126e-08, 1.9952622665186937e-08, 2.0469682837642722e-08, 2.1000142069738104e-08, 2.1544346751056764e-08, 2.2102655705680263e-08, 2.2675431310403837e-08, 2.326305015287744e-08, 2.386589770253522e-08, 2.448436653423869e-08, 2.5118863433704064e-08, 2.5769802292074928e-08, 2.6437611211349576e-08, 2.7122725398953662e-08, 2.7825594273167553e-08, 2.8546676134055815e-08, 2.9286445268894568e-08, 3.004538484674413e-08, 3.082399047116269e-08, 3.1622775509276835e-08, 3.2442258657283674e-08, 3.328297992766238e-08, 3.4145486438319494e-08, 3.503034662344362e-08, 3.5938136022650724e-08, 3.6869451491838845e-08, 3.782490054504706e-08, 3.880510845988283e-08, 3.981071827752203e-08, 4.084238724999523e-08, 4.19007903929014e-08, 4.298662403812159e-08, 4.410059517567788e-08, 4.5243435664588105e-08, 4.641589157472481e-08, 4.761873029224262e-08, 4.885274051957822e-08, 5.0118728722736705e-08, 5.1417522684005235e-08, 5.274997505466672e-08, 5.411695624957247e-08, 5.5519361552569535e-08, 5.695811111650073e-08, 5.843414285777726e-08, 5.994842666723343e-08, 6.150195019927196e-08, 6.309573308271865e-08, 6.473081981539508e-08, 6.640827621140488e-08, 6.812920361198849e-08, 6.989473178009575e-08, 7.170601179495861e-08, 7.356423026294578e-08, 7.547060221213542e-08, 7.742637819774245e-08, 7.943283719669125e-08, 8.149128660761562e-08, 8.360308356714086e-08, 8.576960652817434e-08, 8.799226947076022e-08, 9.027253611293418e-08, 9.261189148901394e-08, 9.501187037130876e-08, 9.747404305926466e-08, 1.0000002248489182e-07, 1.0259145710733719e-07, 1.0525005222916661e-07, 1.0797754157465533e-07, 1.1077570860607011e-07, 1.1364639362909656e-07, 1.1659146537112974e-07, 1.1961286361383827e-07, 1.2271254945517285e-07, 1.2589256925821246e-07, 1.2915499780774553e-07, 1.3250196673197934e-07, 1.3593566450254002e-07, 1.394583506453273e-07, 1.4307232731880504e-07, 1.4677995352485596e-07, 1.5058365931963635e-07, 1.544859458135761e-07, 1.5848935674966924e-07, 1.6259650692518335e-07, 1.6681009640251432e-07, 1.7113286787662219e-07, 1.7556766351845e-07, 1.8011738234235963e-07, 1.847850086278413e-07, 1.8957359770865878e-07, 1.944862759728494e-07, 1.995262692844335e-07, 2.0469686035085033e-07, 2.100014455663768e-07, 2.154435065904181e-07, 2.2102658192579838e-07, 2.2675435218388884e-07, 2.3263054060862487e-07, 2.386590267633437e-07, 2.448437328439468e-07, 2.511887089440279e-07, 2.576981046331639e-07, 2.6437618316776934e-07, 2.712273214910965e-07, 2.7825601023323543e-07, 2.8546682528940437e-07, 2.928645130850782e-07, 3.0045390531086014e-07, 3.0823997576590045e-07, 3.162278119361872e-07, 3.244226434162556e-07, 3.3282984190918796e-07, 3.414549212266138e-07, 3.5030350886700035e-07, 3.593814028590714e-07, 3.686945433400979e-07, 3.7824904097760736e-07, 3.880511201259651e-07, 3.981072325132118e-07, 4.084239151325164e-07, 4.1900796077243285e-07, 4.298662759083527e-07, 4.4100599438934296e-07, 4.5243439217301784e-07, 4.6415894416895753e-07, 4.7618732423870824e-07, 4.885274051957822e-07, 5.01187287227367e-07, 5.141752126291976e-07, 5.274997647575219e-07, 5.411695838120067e-07, 5.551936510528321e-07, 5.695811182704347e-07, 5.843414783157641e-07, 5.994843377266079e-07, 6.150195872578479e-07, 6.309574018814601e-07, 6.473082407865149e-07, 6.640828473791771e-07, 6.812921355958679e-07, 6.989474172769405e-07, 7.17060174793005e-07, 7.35642345262022e-07, 7.547060363322089e-07, 7.742637535557151e-07, 7.943282867017842e-07, 8.149128234435921e-07, 8.360307788279897e-07, 8.576959658057604e-07, 8.799225952316192e-07, 9.027252190207946e-07, 9.26118786992447e-07, 9.501185900262499e-07, 9.7474026006239e-07, 9.999999974752427e-07, 1.0259143436996965e-06, 1.0525002380745718e-06, 1.0797750746860402e-06, 1.1077568160544615e-06, 1.1364636520738713e-06, 1.1659143410724937e-06, 1.196128323499579e-06, 1.227125267178053e-06, 1.2589254083650303e-06, 1.2915496654386516e-06, 1.3250194115244085e-06, 1.3593564744951436e-06, 1.3945833643447259e-06, 1.4307231595012126e-06, 1.4677993931400124e-06, 1.5058365079312352e-06, 1.5448592876055045e-06, 1.5848933117013075e-06, 1.625964841878158e-06, 1.6681007082297583e-06, 1.711328422970837e-06, 1.7556764078108245e-06, 1.8011736528933397e-06, 1.847849944169866e-06, 1.8957357497129124e-06, 1.944862560776528e-06, 1.9952624370489502e-06, 2.0469683477131184e-06, 2.1000141714466736e-06, 2.1544346964219585e-06, 2.2102653929323424e-06, 2.267543095513247e-06, 2.326305093447445e-06, 2.3865898128860863e-06, 2.448436816848698e-06, 2.511886577849509e-06, 2.5769804778974503e-06, 2.643761263243505e-06, 2.7122725896333577e-06, 2.782559477054747e-06, 2.8546676276164362e-06, 2.9286445624165935e-06, 3.0045384846744128e-06, 3.082399189224816e-06, 3.1622776077711023e-06, 3.244226036258624e-06, 3.3282981348747853e-06, 3.4145489280490438e-06, 3.503034804452909e-06, 3.5938137443736196e-06, 3.6869450923404656e-06, 3.7824900118721416e-06, 3.880511030729394e-06, 3.9810720409150235e-06, 4.084239208168583e-06, 4.190079835098004e-06, 4.298663043300621e-06, 4.410060228110524e-06, 4.524344149103854e-06, 4.641589839593507e-06, 4.761873697134433e-06, 4.885274847765686e-06, 5.011873781768372e-06, 5.1417532631603535e-06, 5.2749983296962455e-06, 5.411696747614769e-06, 5.551937647396699e-06, 5.695812433259562e-06, 5.843415692652343e-06, 5.994844286760781e-06, 6.150196895760018e-06, 6.309575383056654e-06, 6.47308388579404e-06, 6.640829724346986e-06, 6.812922492827056e-06, 6.9894749685772695e-06, 7.170602657424752e-06, 7.356424248428084e-06, 7.547061159129953e-06, 7.742638445051853e-06, 7.943283890199382e-06, 8.149128916556947e-06, 8.36030812934041e-06, 8.576959771744441e-06, 8.799226634437218e-06, 9.027253327076323e-06, 9.261189006792847e-06, 9.501186468696687e-06, 9.747403964865953e-06, 1.0000001566368155e-05, 1.025914571073372e-05, 1.0525004654482473e-05, 1.0797753020597156e-05, 1.1077569979534019e-05, 1.1364638339728117e-05, 1.1659145457088016e-05, 1.1961285053985193e-05, 1.2271254490769934e-05, 1.2589256584760733e-05, 1.2915499610244296e-05, 1.325019638898084e-05, 1.359356701868819e-05, 1.3945835235062987e-05, 1.4307232959254179e-05, 1.4677995750389528e-05, 1.5058366443554405e-05, 1.54485951497918e-05, 1.5848936527618207e-05, 1.6259651602013037e-05, 1.6681009583408013e-05, 1.7113286958192475e-05, 1.7556767488713376e-05, 1.801174039428588e-05, 1.8478503989172168e-05, 1.8957362044602633e-05, 1.9448629245744087e-05, 1.995262755372096e-05, 2.0469688024604693e-05, 2.1000147171434946e-05, 2.1544352421187796e-05, 2.2102660295786336e-05, 2.267543641210068e-05, 2.326305548194796e-05, 2.3865903131081723e-05, 2.448437226121314e-05, 2.5118868506979197e-05, 2.576980841695331e-05, 2.6437615815666504e-05, 2.7122729079565033e-05, 2.7825597499031574e-05, 2.854667945939582e-05, 2.928644789790269e-05, 3.0045388484722935e-05, 3.0823994165984914e-05, 3.162277789670043e-05, 3.244226172682829e-05, 3.328298407723196e-05, 3.414549064473249e-05, 3.503034895402379e-05, 3.5938137443736196e-05, 3.686945274239406e-05, 3.782490239245817e-05, 3.880511212628335e-05, 3.981072222813964e-05, 4.084239117219113e-05, 4.1900795622495934e-05, 4.2986626795027405e-05, 4.410059773363173e-05, 4.524343603407033e-05, 4.641589111997746e-05, 4.761873060488142e-05, 4.8852740292204544e-05, 5.0118727813242e-05, 5.1417522627161816e-05, 5.274997602100484e-05, 5.411695747170597e-05, 5.5519365560030565e-05, 5.6958113418659195e-05, 5.843414692208171e-05, 5.9948430134681985e-05, 6.150195258669555e-05, 6.30957365501672e-05, 6.473082612501457e-05, 6.640828360104933e-05, 6.812921492382884e-05, 6.989474059082568e-05, 7.17060174793005e-05, 7.356423157034442e-05, 7.547060522483662e-05, 7.742637535557151e-05, 7.94328298070468e-05, 8.149127825163305e-05, 8.360307401744649e-05, 8.57695922604762e-05, 8.799225906841457e-05, 9.027252235682681e-05, 9.261188097298145e-05, 9.501185559201986e-05, 9.74740250967443e-05, 0.00010000000474974513, 0.00010259143891744316, 0.0001052500301739201, 0.00010797751747304574, 0.00011077568342443556, 0.00011364636156940833, 0.00011659143638098612, 0.00011961282871197909, 0.00012271251762285829, 0.00012589253310579807, 0.0001291549560846761, 0.00013250192569103092, 0.00013593562471214682, 0.00013945830869488418, 0.0001430722768418491, 0.00014677990111522377, 0.00015058361168485135, 0.0001544858969282359, 0.00015848930343054235, 0.00016259645053651184, 0.0001668100303504616, 0.00017113280773628503, 0.00017556760576553643, 0.00018011733482126147, 0.00018478496349416673, 0.00018957354768645018, 0.00019448623061180115, 0.00019952621369156986, 0.0002046968147624284, 0.00021000140986870974, 0.00021544346236623824, 0.00022102653747424483, 0.00022675430227536708, 0.00023263049661181867, 0.0002386589621892199, 0.00024484365712851286, 0.00025118861231021583, 0.00025769800413399935, 0.0002643760817591101, 0.0002712272107601166, 0.0002782559022307396, 0.00028546672547236085, 0.0002928644244093448, 0.00030045383027754724, 0.00030823989072814584, 0.00031622772803530097, 0.00032442258088849485, 0.00033282977528870106, 0.0003414548409637064, 0.0003503034240566194, 0.00035938131622970104, 0.00036869445466436446, 0.00037824895116500556, 0.0003880510339513421, 0.000398107134969905, 0.0004084238316863775, 0.0004190078761894256, 0.0004298661951906979, 0.0004410059191286564, 0.00045243429485708475, 0.00046415883116424084, 0.0004761872114613652, 0.0004885272937826812, 0.0005011871689930558, 0.0005141751025803387, 0.0005274996510706842, 0.0005411694874055684, 0.0005551935755647719, 0.0005695810541510582, 0.0005843414110131562, 0.0005994842504151165, 0.0006150195258669555, 0.000630957365501672, 0.0006473082466982305, 0.0006640828214585781, 0.0006812920910306275, 0.0006989473477005959, 0.0007170601165853441, 0.0007356422720476985, 0.00075470597948879, 0.0007742636953480542, 0.0007943282253108919, 0.0008149127243086696, 0.0008360306965187192, 0.0008576958789490163, 0.0008799225324764848, 0.000902725150808692, 0.0009261186933144927, 0.0009501184686087072, 0.0009747401927597821, 0.0009999999310821295, 0.0010259143309667706, 0.0010525002144277096, 0.0010797750437632203, 0.0011077567469328642, 0.001136463601142168, 0.001165914349257946, 0.0011961283162236214, 0.0012271251762285829, 0.0012589253019541502, 0.0012915495317429304, 0.0013250191695988178, 0.0013593562180176377, 0.001394583028741181, 0.0014307227684184909, 0.0014677990693598986, 0.001505836145952344, 0.001544858911074698, 0.0015848929760977626, 0.001625964418053627, 0.0016681002452969551, 0.0017113280482590199, 0.0017556759994477034, 0.0018011732026934624, 0.0018478494603186846, 0.0018957352731376886, 0.0019448620732873678, 0.0019952619913965464, 0.0020469678565859795, 0.0021000136621296406, 0.002154434099793434, 0.002210264792665839, 0.0022675422951579094, 0.0023263043258339167, 0.0023865890689194202, 0.0024484361056238413, 0.002511885715648532, 0.002576979575678706, 0.0026437602937221527, 0.0027122716419398785, 0.0027825585566461086, 0.002854666905477643, 0.0029286437202244997, 0.0030045376624912024, 0.0030823983252048492, 0.0031622766982764006, 0.0032442251686006784, 0.0033282972872257233, 0.0034145480021834373, 0.0035030338913202286, 0.003593812696635723, 0.0036869440227746964, 0.0037824888713657856, 0.00388050964102149, 0.003981070592999458, 0.004084237385541201, 0.004190078005194664, 0.00429866136983037, 0.004410058259963989, 0.0045243422500789165, 0.004641587845981121, 0.004761871881783009, 0.004885272588580847, 0.0050118714570999146, 0.005141750909388065, 0.0052749961614608765, 0.0054116942919790745, 0.0055519347079098225, 0.005695809610188007, 0.005843413062393665, 0.005994841456413269, 0.006150193978101015, 0.0063095721416175365, 0.006473080720752478, 0.006640826351940632, 0.006812918931245804, 0.006989471614360809, 0.007170599419623613, 0.007356421090662479, 0.007547058165073395, 0.007742635440081358, 0.007943280972540379, 0.00814912561327219, 0.008360304869711399, 0.008576956577599049, 0.008799223229289055, 0.009027249179780483, 0.009261184372007847, 0.009501182474195957, 0.00974739994853735, 0.009999997913837433, 0.010259141214191914, 0.010525000281631947, 0.010797749273478985, 0.01107756607234478, 0.011364634148776531, 0.011659141629934311, 0.011961281299591064, 0.01227125059813261, 0.012589252553880215, 0.012915494851768017, 0.013250191695988178, 0.013593561947345734, 0.013945830054581165, 0.014307226985692978, 0.014677989296615124, 0.015058360062539577, 0.015448587946593761, 0.015848929062485695, 0.01625964418053627, 0.01668100245296955, 0.017113279551267624, 0.01755675859749317, 0.018011730164289474, 0.01847849227488041, 0.01895735040307045, 0.019448617473244667, 0.019952615723013878, 0.020469674840569496, 0.021000133827328682, 0.021544339135289192, 0.02210264652967453, 0.022675422951579094, 0.02326304279267788, 0.023865889757871628, 0.024484358727931976, 0.025118855759501457, 0.0257697943598032, 0.026437602937221527, 0.02712271735072136, 0.02782558463513851, 0.028546666726469994, 0.02928643487393856, 0.030045373365283012, 0.030823979526758194, 0.031622763723134995, 0.03244224935770035, 0.033282969146966934, 0.034145474433898926, 0.03503033146262169, 0.03593812137842178, 0.036869436502456665, 0.03782488405704498, 0.03880509361624718, 0.039810702204704285, 0.04084237292408943, 0.04190077632665634, 0.042986609041690826, 0.044100578874349594, 0.04524341598153114, 0.046415869146585464, 0.04761870577931404, 0.04885271564126015, 0.05011870339512825, 0.05141749605536461, 0.05274994671344757, 0.054116927087306976, 0.055519331246614456, 0.05695807933807373, 0.05843411013484001, 0.0599483922123909, 0.061501916497945786, 0.06309569627046585, 0.06473077833652496, 0.06640823185443878, 0.06812915951013565, 0.0698946863412857, 0.07170595973730087, 0.07356417179107666, 0.07547053694725037, 0.07742630690336227, 0.07943276315927505, 0.08149120956659317, 0.08360300213098526, 0.08576951920986176, 0.08799218386411667, 0.0902724489569664, 0.09261180460453033, 0.09501177817583084, 0.09747394919395447, 0.09999992698431015, 0.10259135812520981, 0.10524994879961014, 0.10797743499279022, 0.11077560484409332, 0.11364628374576569, 0.11659135669469833, 0.11961274594068527, 0.12271243333816528, 0.12589244544506073, 0.12915486097335815, 0.13250182569026947, 0.13593553006649017, 0.13945820927619934, 0.14307217299938202, 0.14677979052066803, 0.15058349072933197, 0.1544857621192932, 0.1584891676902771, 0.16259631514549255, 0.16680990159511566, 0.1711326688528061, 0.17556746304035187, 0.1801171898841858, 0.18478481471538544, 0.18957339227199554, 0.19448606669902802, 0.19952605664730072, 0.2046966403722763, 0.21000123023986816, 0.21544328331947327, 0.22102636098861694, 0.2267541140317917, 0.23263029754161835, 0.23865877091884613, 0.2448434680700302, 0.2511884272098541, 0.25769782066345215, 0.2643758952617645, 0.27122703194618225, 0.2782557010650635, 0.2854665219783783, 0.29286420345306396, 0.3004536032676697, 0.3082396686077118, 0.3162274956703186, 0.32442232966423035, 0.3328295350074768, 0.3414545953273773, 0.35030317306518555, 0.3593810498714447, 0.3686941862106323, 0.3782486617565155, 0.38805073499679565, 0.39810681343078613, 0.408423513174057, 0.4190075397491455, 0.42986586689949036, 0.44100555777549744, 0.4524339437484741, 0.46415847539901733, 0.4761868417263031, 0.4885269105434418, 0.5011867880821228, 0.5141746997833252, 0.5274991989135742, 0.5411689877510071, 0.5551930665969849, 0.5695805549621582, 0.5843408703804016, 0.5994836688041687, 0.615018904209137, 0.6309567093849182, 0.6473075747489929, 0.6640821695327759, 0.6812914609909058, 0.6989467144012451, 0.7170594930648804, 0.7356416583061218, 0.7547053694725037, 0.7742630839347839, 0.7943276166915894, 0.8149121403694153, 0.8360300660133362, 0.8576952815055847, 0.8799219131469727, 0.9027245044708252, 0.926118016242981, 0.9501177668571472, 0.9747394919395447, 0.9999992251396179, 1.0259135961532593, 1.052499532699585, 1.0797743797302246, 1.1077560186386108, 1.1364628076553345, 1.165913462638855, 1.1961274147033691, 1.2271243333816528, 1.2589244842529297, 1.291548728942871, 1.3250184059143066, 1.3593554496765137, 1.3945822715759277, 1.4307219982147217, 1.4677982330322266, 1.5058352947235107, 1.5448580980300903, 1.5848921537399292, 1.6259636878967285, 1.6680995225906372, 1.711327314376831, 1.7556753158569336, 1.8011724948883057, 1.8478487730026245, 1.8957345485687256, 1.9448612928390503, 1.9952611923217773, 2.046967029571533, 2.10001277923584, 2.154433250427246, 2.210263967514038, 2.2675416469573975, 2.326303482055664, 2.3865880966186523, 2.448435068130493, 2.5118846893310547, 2.5769786834716797, 2.6437594890594482, 2.712270736694336, 2.782557487487793, 2.854665756225586, 2.928642511367798, 3.0045363903045654, 3.082396984100342, 3.1622753143310547, 3.2442235946655273, 3.3282957077026367], \"y\": [1.0084890477573714, 1.0142189587106827, 1.019596095467159, 1.0246295105180045, 1.0293282563544244, 1.033701385467623, 1.0377579503488046, 1.0415070034891745, 1.044957597379937, 1.048118784512297, 1.0509996173774587, 1.053609148466627, 1.0559564302710067, 1.058050515281802, 1.059900455990218, 1.061515304887459, 1.06290411446473, 1.0640759372132353, 1.0650398256241798, 1.0658048321887679, 1.0663800093982043, 1.066774409743694, 1.066997085716441, 1.0670570898076503, 1.0669634745085268, 1.0667252923102746, 1.066351595703951, 1.0648570361668508, 1.0613749403245314, 1.058172710246506, 1.0565282253620518, 1.0560977984214188, 1.0561202116803026, 1.0569710018005225, 1.0583526116423474, 1.0598466339197419, 1.062272161482751, 1.064378398672935, 1.064468892235599, 1.0644967059964467, 1.0643124539847535, 1.0637456035169908, 1.0621465967419543, 1.060094889664813, 1.0574880160976978, 1.0576749846642968, 1.0588762041970319, 1.0607143296997665, 1.0621244833886732, 1.0623983043603362, 1.0626233730025052, 1.0624205513083689, 1.0623071630188914, 1.0620449972540942, 1.0631073995958618, 1.0664371185423458, 1.067738504128668, 1.0688021133276744, 1.0688667822503315, 1.0687572741685014, 1.0682114812982546, 1.0664796452084901, 1.0640725556696777, 1.0611364794129614, 1.0552545840930465, 1.0494982804686264, 1.0450242972916175, 1.0424808522799742, 1.0402355081809178, 1.0385406879768184, 1.0373757890404542, 1.0368127145278396, 1.0355676496355823, 1.0355673348633996, 1.0347032682370525, 1.0343740098939975, 1.032461020957425, 1.0312577794373992, 1.030050479473134, 1.0291700028719821, 1.0293737494815398, 1.0330302936675013, 1.0380360795886978, 1.0433806659485632, 1.0480605978774742, 1.0526854699691657, 1.0580852778824146, 1.0633074214473586, 1.0683737652065277, 1.0732783557799228, 1.0753770045308406, 1.0754002825652558, 1.076150965484785, 1.0777150883332491, 1.0784512252056055, 1.0801226632130005, 1.0825641826342445, 1.0855238789168733, 1.0887070487994865, 1.0911851596991935, 1.0944718839528687, 1.0968443229800497, 1.0998425152992832, 1.102205461798147, 1.1039630028194676, 1.1052370101299123, 1.1063465992756467, 1.1065870004263554, 1.1044378004656161, 1.102419445058906, 1.0978771493900308, 1.0946692619946703, 1.0907852535842393, 1.085445695863357, 1.0799984907166664, 1.0744976871561347, 1.0686854886160413, 1.0665276979607161, 1.066687122541985, 1.0655241025922817, 1.0635750701238105, 1.0620164586391827, 1.0596476384577023, 1.0575246827389946, 1.0554363215296534, 1.0539475983463402, 1.0515510399878254, 1.0516629644603575, 1.049776047789788, 1.0498742188348391, 1.0491085037333525, 1.0486506690603237, 1.0484898383001795, 1.0481641455068549, 1.0475746514324018, 1.0466757036820018, 1.0466162906489296, 1.0464610934749203, 1.0458304697719345, 1.0452259080811008, 1.0441915453916208, 1.0421200977865634, 1.0395130881890315, 1.0369504909017906, 1.036615548864045, 1.0344274546744767, 1.032170849471266, 1.032142391906884, 1.0316492459953244, 1.0325298075560985, 1.033684816998775, 1.0352676069863742, 1.0368206534082578, 1.036562751002346, 1.0393153396313828, 1.03962587142912, 1.0397090266322213, 1.0405614475718465, 1.040757087222022, 1.0403739090955961, 1.0406082121156053, 1.0402947426526434, 1.0400687588798134, 1.039077761597451, 1.037712218746626, 1.0367881670365746, 1.036768484784212, 1.0368772858735256, 1.0377893709362773, 1.0391807983444816, 1.0410926788126753, 1.0416143067354822, 1.0428216796313914, 1.043128346418831, 1.0444244613632538, 1.0469712341976567, 1.0493899991924363, 1.0513101085295826, 1.0530793451176492, 1.054774882748536, 1.054412821782173, 1.0547864945704855, 1.0559398158263156, 1.0580004008682542, 1.060196806378564, 1.0617994472327348, 1.063506425443249, 1.065501397264023, 1.0674983176172264, 1.0674468300353344, 1.06948284259187, 1.0699502970071249, 1.068354805583053, 1.066552699724653, 1.0661445292161134, 1.0649827438842214, 1.0635709804248707, 1.0629180443055684, 1.0642462942182844, 1.0656736201490853, 1.0676410172771014, 1.0693049565077917, 1.0706997069576343, 1.0715299946617307, 1.0712260807657183, 1.071050275359449, 1.0714391602682694, 1.0688927821199352, 1.0682982242848036, 1.0670975719854947, 1.0655346403578974, 1.0639340733556584, 1.0622625283774416, 1.0602917306634128, 1.0583697350867347, 1.0560523874160899, 1.0561883029836685, 1.0569255473020254, 1.0572923956562033, 1.0574537697862656, 1.057644185336802, 1.0573349508887955, 1.057243016428498, 1.0563159788330585, 1.054896283932594, 1.053781869408676, 1.0525632190926604, 1.05077181580723, 1.0492083992837293, 1.0473983064329992, 1.0464535767284162, 1.0457261360660826, 1.045236997323492, 1.0450156736584142, 1.0454389745453236, 1.0433963931010735, 1.0413917440142688, 1.03866639066048, 1.0375101391727155, 1.0368530020896634, 1.036728902037841, 1.036695213926824, 1.0370201161031154, 1.0348001899661026, 1.0342240831513978, 1.0359425937691369, 1.0371120696277583, 1.0376859145571518, 1.0388955070780719, 1.0409927223947162, 1.042676448064114, 1.0444209022845758, 1.0457990868830331, 1.0478458554769006, 1.0496587558802035, 1.0500686211488797, 1.0506775788250977, 1.0523372434627714, 1.0539342088133044, 1.0549697376092333, 1.0563863458719518, 1.0578784537402908, 1.0636444697885208, 1.0675716972797031, 1.0701983883718353, 1.072043662438654, 1.07384095520545, 1.0751014366457285, 1.0763948580026694, 1.0775578319213013, 1.0774651390067835, 1.0760370096371252, 1.0743123072208085, 1.0728007791596772, 1.0719850883750863, 1.0711895308619612, 1.0704426700248768, 1.0696418041320714, 1.0679480298809354, 1.0674106390945517, 1.063884634288763, 1.0596917265057355, 1.0570760859259054, 1.054441760291268, 1.052053110881548, 1.0492256484565705, 1.0468930347139223, 1.0446143532096157, 1.0440936889623895, 1.0433076544805338, 1.0447644805728737, 1.0447149145626988, 1.0450822046383532, 1.0459600377786211, 1.0477339929845388, 1.0496625901229335, 1.0518029199551029, 1.0543452773127835, 1.0560168374402386, 1.057527073227446, 1.05999545911005, 1.0624829520851524, 1.0644926913099915, 1.066063873871867, 1.0679141813708182, 1.0700422459731638, 1.0698207928449452, 1.070647871574732, 1.0721129378062402, 1.0739289082907773, 1.0754351955814923, 1.0769220455034623, 1.0798174861988825, 1.0825390682944482, 1.0849382945065862, 1.0829569259259464, 1.0812943344602561, 1.0798534703727103, 1.077865054250973, 1.0767771326174609, 1.0755651670967696, 1.0733569014769753, 1.070722060996769, 1.0681060665658555, 1.0669187871588752, 1.0644525090831847, 1.0623330052257598, 1.0581410524641823, 1.054105407738561, 1.0502301647952492, 1.0468349520774496, 1.04401723290399, 1.0411734505336223, 1.0413733226548358, 1.043676950762772, 1.0458029970944718, 1.0485289999290004, 1.0498234461418312, 1.0515483071831213, 1.0529943251870548, 1.0545327359232601, 1.055311901491519, 1.0564804817348734, 1.0549145150575279, 1.055652120991028, 1.055980652411207, 1.0555957512949805, 1.0553049805689456, 1.055961968017938, 1.057281690160435, 1.0588518171710208, 1.0597902836729347, 1.063406556576941, 1.0666325661297689, 1.06873324955148, 1.069105422487025, 1.0697020754275253, 1.0703340389991436, 1.0706613434873886, 1.070949794251021, 1.071621105692671, 1.0735178116999557, 1.0729175814102787, 1.0723773228472364, 1.0721092502086176, 1.0709599676110027, 1.0695095811236561, 1.0681956423156191, 1.0665337633091616, 1.0642500572794409, 1.0634467480652907, 1.0620790437627705, 1.0599238041393328, 1.0571084318589181, 1.0543572175706992, 1.0517919135914942, 1.0489925961959194, 1.0462004798424966, 1.0449354165304483, 1.0405898402873854, 1.0395633501981512, 1.039650344779929, 1.039591542070514, 1.0393801551750914, 1.039663763486825, 1.0402315661314216, 1.041463112664985, 1.041423816749804, 1.0406968964298247, 1.0398485283414285, 1.0399694732501568, 1.0401589509672236, 1.0397284181503146, 1.038582811350876, 1.0377459192043623, 1.0370067029832148, 1.0385329824030887, 1.042999919716903, 1.0446619764487628, 1.046575135948077, 1.0483782918910505, 1.049546544029208, 1.0514327540380306, 1.0526749054954248, 1.0539458946193463, 1.053200727387712, 1.0509532436617262, 1.0477784683695461, 1.047652996833551, 1.0479881795812058, 1.0478392474982576, 1.047698959032998, 1.0473671278332897, 1.0465310666215912, 1.0442335638817106, 1.0406454610313152, 1.0385543207336103, 1.0369958061463729, 1.0361332778984074, 1.036338998154812, 1.0366279581168771, 1.0367725992547723, 1.0367620402657896, 1.0349369149023324, 1.0327492233914244, 1.030679184184404, 1.0294138377758526, 1.0284399221045704, 1.0272753584240357, 1.0262548633496458, 1.0249556387699834, 1.0237433128309323, 1.0263605395041318, 1.0313563982600136, 1.0308378978159518, 1.030040508856749, 1.0304019532449058, 1.0318595146015244, 1.0330477753772405, 1.0341253616639774, 1.0358455371403, 1.03783677622245, 1.0402296385757857, 1.0414538679791676, 1.0425040693746395, 1.043077939051598, 1.0448965373305779, 1.0469651834331346, 1.0480274449874203, 1.0483883242109626, 1.0443309617623244, 1.0423402989405983, 1.0400585164491685, 1.03756591133686, 1.036278430616275, 1.0345403899662484, 1.0322767353515896, 1.0303911466178985, 1.0285061817092707, 1.0268600305048339, 1.0241142221251547, 1.0212403143729396, 1.019410350586957, 1.0175989175127687, 1.0154354635201057, 1.0139286247165311, 1.012219905305858, 1.009393861376405, 1.0104111035442962, 1.0088367652018433, 1.0070779929606153, 1.0054994859976454, 1.0046033066427367, 1.003830784872439, 1.0039733267974114, 1.0043819648154122, 1.0048282113676221, 1.006763958242829, 1.0073658935705496, 1.0084763391023008, 1.0104912947709708, 1.0131316526109604, 1.0156063304912124, 1.0174748719476863, 1.0191914804850468, 1.0204492681453958, 1.0174463979170028, 1.0178204929534949, 1.0177672081330948, 1.0179387895179801, 1.017934417594566, 1.0185261704571258, 1.018566329987138, 1.0183239890964517, 1.0176199742923824, 1.0159121146235317, 1.0157572245538695, 1.014217007522088, 1.014466490517834, 1.013154330996549, 1.0104215691781282, 1.0073375066765522, 1.0042780892853107, 1.0005933634327255, 0.9971015589674368, 0.9950124589515772, 0.9932981933560714, 0.9903278941462559, 0.9858349293646427, 0.9817906716144941, 0.9768029698243129, 0.9718016519240611, 0.9664223979066813, 0.9641734961061501, 0.9629381614751945, 0.9591603707729432, 0.9544053773394672, 0.950067155344926, 0.9452186799375225, 0.9403668977262508, 0.9358915646131915, 0.9311913433257444, 0.9289401669383577, 0.9314316879088725, 0.9323541792293432, 0.9326775402728469, 0.9319853432758635, 0.9297806948058248, 0.9268668197787973, 0.9232874345361969, 0.9195722752472065, 0.9172200999048427, 0.9173133920051102, 0.9139538199180317, 0.9099394629813685, 0.9059548305460756, 0.9010704503675412, 0.8961488140565098, 0.891235048204744, 0.8862184916923627, 0.8816788190789066, 0.8811864410635301, 0.878951976330677, 0.8753386253472727, 0.8708402807277617, 0.8665625502609504, 0.8629942077605787, 0.8599823068601797, 0.8566890387971242, 0.8518820010066568, 0.847419488593486, 0.8419911437793124, 0.8370480404203533, 0.833233094632648, 0.8303721278540869, 0.827182014400218, 0.824840965424599, 0.8228948527357492, 0.8202009462657186, 0.8126601783264227, 0.8086436757067436, 0.8064519879542844, 0.8042925074281961, 0.8038178020967786, 0.8035808611367272, 0.8044059802382748, 0.8054231239391244, 0.8051372241592673, 0.8098534582155892, 0.8133632322574309, 0.8157336983053176, 0.8174210824526031, 0.8191715513596673, 0.8212751297653235, 0.8232982151382817, 0.8253354063043497, 0.823539547422876, 0.8249246636909052, 0.8245218748485157, 0.8236238284627563, 0.8224203559273716, 0.8214163002482205, 0.8206358193965161, 0.8200777180799205, 0.8195357748662573, 0.8148608296467983, 0.8118710752307463, 0.8069159934250414, 0.8015157392437403, 0.7969590939445723, 0.7928971796339694, 0.788878948287144, 0.7854653722088211, 0.7821690726623127, 0.7762910213759016, 0.7723679425628485, 0.7685635279554413, 0.7646702082241952, 0.76126519439429, 0.7571844328315965, 0.7534171085870937, 0.7503151955197926, 0.7479922705599638, 0.7460653703407276, 0.7468815736735432, 0.744019972798026, 0.7411870826036588, 0.7368174179592076, 0.7331752049826902, 0.7291172012624074, 0.7255621094801776, 0.7217090903250275, 0.7232828917866558, 0.7217985581116612, 0.7203878031621784, 0.7200605405433242, 0.7195119535196718, 0.7206201668992733, 0.7206778169795884, 0.7214669146686886, 0.7223047452560848, 0.7186887183182139, 0.713916993011173, 0.7129712924382792, 0.7129483446560545, 0.7120304264389579, 0.7116236714129344, 0.7115804096388749, 0.7121432738062109, 0.7129575103240707, 0.7109534602786802, 0.7107251121421668, 0.708436013885731, 0.7059391525290861, 0.7030836976135224, 0.7003325597450613, 0.6972027785865363, 0.6929079566757729, 0.6883251205989354, 0.6833254672505532, 0.6816435000234209, 0.678363951763634, 0.6749103802830718, 0.6714782815396713, 0.6684212404799512, 0.6655568105037382, 0.6631411466283899, 0.6609902451417767, 0.6576807168823116, 0.6567841278364316, 0.6553936097313331, 0.6522817850809632, 0.6491248990244576, 0.6459932156652743, 0.6431289526794933, 0.6397186971229474, 0.6364168337044647, 0.6296600249216654, 0.6257609112412688, 0.6220680200875943, 0.6190754675489041, 0.6151181894778687, 0.6122011883372717, 0.6091469459835468, 0.6062652032966992, 0.6034680824264032, 0.6020811138617964, 0.6033973729750332, 0.6030762018775044, 0.6029185863041205, 0.6009002494866504, 0.5987949246306228, 0.5969465476401759, 0.5953799444241948, 0.5940759191176896, 0.5961713347721878, 0.5982198339304748, 0.5985955250585935, 0.6007124424256521, 0.6013571232505344, 0.6015177366350467, 0.6012455662688129, 0.6004112263329281, 0.5991943385607188, 0.5957109370392938, 0.5928283639228058, 0.5888471681213754, 0.5848662191099968, 0.5808890438646748, 0.577777681299148, 0.5755465958026467, 0.5736235491731427, 0.5716841919860858, 0.5681861527161842, 0.5675001362174932, 0.5679379106950361, 0.5685386201913829, 0.5687220246696509, 0.5686729927377813, 0.5677413987578573, 0.5662850960629782, 0.5649920030938915, 0.561033253898914, 0.5510621792900567, 0.5463085798451005, 0.5447115996279868, 0.5444694860901401, 0.5447609640372232, 0.5457273429173763, 0.5466647513120555, 0.5480639060826715, 0.5502719205222021, 0.5510278091325496, 0.5528567840804224, 0.5569532139487441, 0.5608701680372384, 0.564826739861492, 0.5690388204699174, 0.5729802925149771, 0.5766955488276514, 0.5774554257290565, 0.5768048558918999, 0.5786462465095312, 0.5820644586035669, 0.586538545335452, 0.5908774533457163, 0.5954565683871524, 0.6000297521079019, 0.6040909113621886, 0.6044348788578113, 0.6101075501502933, 0.6103590799229928, 0.6085806771226441, 0.6070780855574577, 0.605850705686858, 0.6049833108979609, 0.6040227164618487, 0.6023269492559536, 0.6005068683031658, 0.5931530636712353, 0.590051625682001, 0.5874117429140635, 0.5837581843398538, 0.5753845985541652, 0.5689321019107216, 0.5650731891456022, 0.5633471940674593, 0.5621917814138847, 0.5657587429069323, 0.5688610130818916, 0.5716453398070678, 0.5714150538520464, 0.5681803908877486, 0.5671823425430982, 0.5671133870806351, 0.5696806812722199, 0.5819920776481556, 0.6011665321185825, 0.6209107781619987, 0.6391139199599093, 0.6557242512617836, 0.6690686686062577, 0.6803263701928315, 0.6916110421083684, 0.699410478566611, 0.710926733856061, 0.721299028224506, 0.7245711626401478, 0.7360608908556782, 0.7442881645767734, 0.7539906696141095, 0.7644628586007634, 0.7766881785686606, 0.7902017332489834, 0.8088608397001196, 0.8162866985474397, 0.7958527103692294, 0.7965789710658766, 0.8064628480035552, 0.8331584902303362, 0.8630101041286964, 0.9028226394288097, 0.9514835809688131, 0.9977792835074234, 1.005827250796607, 1.0472588805301228, 1.097890383545647, 1.1542529806744188, 1.2104714578014968, 1.2613135391677093, 1.3069417828711152, 1.3590260230378017, 1.423373434277834, 1.48158128295705, 1.568592970894967, 1.6645244577854599, 1.7641856174884634, 1.8653391375872033, 1.9655175059655197, 2.0606105384615954, 2.1481383458632393, 2.2263409194530883, 2.314922234284281, 2.393137513682773, 2.4659134806516128, 2.534100780103258, 2.589251670300836, 2.634953645515144, 2.670494127352574, 2.697004555356488, 2.706463874582792, 2.6917873097886442, 2.6628471569986725, 2.626004084497822, 2.579151467891244, 2.517123167046082, 2.4584866710196978, 2.4042914234870483, 2.3457364299654677, 2.2833432719557822, 2.2379814639192785, 2.1761875482993425, 2.108252653415234, 2.018482610070332, 1.9147429108171994, 1.813978491679834, 1.7083367682188917, 1.6165453587320886, 1.541903897558907, 1.5185807607400577, 1.5199916667910132, 1.4858536209565862, 1.4456047182088296, 1.41232851074766, 1.3866621064172593, 1.3692426130616069, 1.3607071385246798, 1.3616927906504575, 1.3728366772829177, 1.3947759062660379, 1.428147585443798, 1.4735888226601745, 1.5317367257591477, 1.6032284025846937, 1.6887009609807915, 1.7887915087914195, 1.9041371538605576, 2.035375004032181, 2.1831421671502698, 2.3480757510588033, 2.530812863601757, 2.73199061262311, 2.9522461059668434, 3.192216451476931, 3.4525387569973534, 3.733850130372091, 4.036787679445117, 4.361988512060413]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"title\": {\"text\": \"Learning Rate\"}, \"type\": \"log\"}, \"yaxis\": {\"title\": {\"text\": \"Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3e8dc70b-8d19-44c9-93ea-2cf71af7d4e7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSQg64F6GR-L"
      },
      "source": [
        "# save the model and label binarizer to disk\n",
        "save = False\n",
        "if save:\n",
        "  print(\"[INFO] serializing network and label binarizer...\")\n",
        "  model.save(os.path.join(output_path,'smallvggnet.model'), save_format=\"h5\")\n",
        "  f = open(os.path.join(output_path,'smallvggnet.pickle'), \"wb\")\n",
        "  f.write(pickle.dumps(classes))\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}